<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><p><img src="./assets/nlpwme1c.png" alt=intro  /></p> <h1 id=understanding_natural_language_processing_with_me ><a href="#understanding_natural_language_processing_with_me" class=header-anchor >Understanding Natural Language Processing with Me&#33;</a></h1> <p>Hi, I‚Äôm Shiyi. Welcome to my technical blog. Please check out this <a href="https://shiyis.github.io/nlp-docs">page</a> for a more detailed account of my journey with respect to learning Natural Language Processing. Here I will only be documenting the gists. I will be presenting everything I have learned so far, including important concepts, necessary code snippets, and more. I am by no means an expert in this subject, but I have gone through extensive studies and training in the fields and subfields related to have a good grasp of what‚Äôs important.</p> <p>Areas that I have dabbled in,</p> <pre><code class="julia hljs">‚Üí General Linguistics
‚Üí Symbolic Computational Linguistics
‚Üí Statistical Natural Language Processing
‚Üí State of the Art Large Language Modeling</code></pre> <h3 id=the_subject_matter ><a href="#the_subject_matter" class=header-anchor ><strong>The Subject Matter</strong></a></h3> <p>What do we mean by Natural Language Processing? If we do a little googling and researching, it&#39;s very intuitive that natural language processing involves a set of solutions to various natural human language tasks. The most common ones are</p> <pre><code class="plaintext hljs">‚Üí Sentiment analysis
‚Üí Machine translation
‚Üí Word-sense disambiguation
‚Üí Named-entity recognition
‚Üí Topic modeling
‚Üí Text classification
‚Üí Document Classification
‚Üí Question answering</code></pre> <h3 id=a_little_bit_of_history ><a href="#a_little_bit_of_history" class=header-anchor ><strong>A Little Bit Of History</strong></a></h3> <p>The history of Computational Linguistics dates back to the 40s to 50s. So, it&#39;s not very long ago that the field that has created ChatGPT or any form of AI that is so commonly adopted in every aspect of our lives now started to have its very first ancestral ideation. It&#39;s still a fairly new and young field with infinite possibilities up for exploration.</p> <p>Before diving in, first we have to ask ourselves what exactly is artificial intelligence &#40;AI&#41;?</p> <p>According to the official definition extracted out of John McCarthy&#39;s 2004 <a href="https://www-formal.stanford.edu/jmc/whatisai.pdf">paper</a> listed on IBM&#39;s <a href="https://www.ibm.com/topics/artificial-intelligence">website</a>,</p> <pre><code class="plaintext hljs">ü§ñÔ∏è &quot;It is the science and engineering of making intelligent machines,
 especially intelligent computer programs. It is related to the similar
 task of using computers to understand human intelligence, but AI does
 not have to confine itself to methods that are biologically observable.&quot;</code></pre> <p>So if it&#39;s to understand human intelligence, we need to know how we as humans gain information and human intelligence, or the brain, really works both through physiology and psychology,</p> <pre><code class="plaintext hljs">üí° Two Important Sources of Knowledge: Rationalism and Empiricism.

  The first acquires knowledge through reasoning and logic, while the second
  through experience and experimentation.</code></pre> <p>Below are some important notes with respect to the <strong>historical timeline</strong> of the development of Computational Linguistics and NLP and how it all started from one of these two principles and gradually transitioned to the other &#40;rationalism / computationalism to empiricism / connectionism; although computationalism is not always symbolic; namely it also incorporates empirical evidence&#41;:</p> <div class=cards ><div class=column ><div class=row ><div class=card ><div class=container ><h2> Noisy Channel Model </h2> <div class=content ><p>Shannon used metaphors like noisy channels and decoding to explain language transmission. He created the first probabilistic measurement of English entropy.</p> <p>Instrumental phonetics and sound spectrograph laid the groundwork for speech recognition research.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Foundational Insights: 1940s - 1950s </h2> <div class=content ><p>Chomsky&#39;s work on finite-state machines and grammar paved the way for formal language theory and transformational and context-free grammar.</p> <p>Turing&#39;s work on computer science led to the development of propositional logic, regular expressions, and finite state automata.</p> <p>Probabilistic models like Markov processes have significantly automated and formalized natural language processing, providing a probabilistic framework for understanding language structure.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> The Merging of Two Cultures </h2> <div class=content ><p><em>Language theory</em>: involves the study of parsing algorithms, formal language theory, and generative syntax. The Transformations and Discourse Analysis Project &#40;TDAP&#41; was one of the earliest complete parsing systems.</p> <p><em>Artificial intelligence &#40;AI&#41;</em>: was coined in 1956 by John McCarthy, Marvin Minsky, Cloude Shannon, and Nathaniel Rochester. Early systems used keyboard searches, pattern matching, and basic reasoning. By the late 1960s, more formal logical systems had been developed.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Paradigms Develop </h2> <div class=content ><p>The Bayesian approach was used to address optical character recognition issues, with Bledsoe and Browning creating a text-recognition system using a large dictionary.</p> <p>Mosteller and Wallace addressed authorship attribution issues.</p> <p>The first online corpus was created in 1963-1964, containing 1 million words from 500 different texts. IBM and Carnegie Mellon University employees developed speech recognition algorithms with techniques such as the HMM and analogies to a noisy channel and decoding.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Empiricism Redux </h2> <div class=content ><p>Kaplan and Kay&#39;s work in phonology and syntax led to the two-level morphology model, which was influential in language modeling.</p> <p>IBM&#39;s research in speech recognition, particularly through Statistical Machine Translation &#40;SMT&#41; and Hidden Markov Models &#40;HMMs&#41;, introduced empiricism to computational linguistics.</p> <p><a href="https://en.wikipedia.org/wiki/Connectionism">Connectionist strategies</a>, inspired by neural networks, predated the neural language models we use today. Modern neural language models, like transformers in GPT or BERT, differ significantly in their complexity and capacity to learn from vast datasets.</p> <p>The transition to probabilistic techniques was more a result of computational needs than a direct evolution.</p></div></div></div></div></div></div> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ¬©Ô∏è Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>