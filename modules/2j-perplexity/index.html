<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=perplexity_and_how_its_applied_in_natural_language_processing ><a href="#perplexity_and_how_its_applied_in_natural_language_processing" class=header-anchor >Perplexity and How It&#39;s Applied in Natural Language Processing </a></h2> <p>Perplexity is a measure used in natural language processing &#40;NLP&#41; to evaluate the performance of a language model. It quantifies how well the model predicts a sample of text.</p> <p>Here&#39;s a breakdown of perplexity:</p> <h3 id=definition ><a href="#definition" class=header-anchor >Definition:</a></h3> <p>Perplexity measures how well a probability model predicts a sample. <code>It reflects how surprised the model is when it sees new data. A lower perplexity indicates that the model is better at predicting the sample.</code></p> <h3 id=formula ><a href="#formula" class=header-anchor >Formula:</a></h3> <p>Perplexity is calculated as the inverse probability of the test set, normalized by the number of words:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mtext>Perplexity</mtext><mo stretchy=false >(</mo><msubsup><mi>w</mi><mn>1</mn><mi>N</mi></msubsup><mo stretchy=false >)</mo><mo>=</mo><mroot><mfrac><mn>1</mn><mrow><mi>P</mi><mo stretchy=false >(</mo><msubsup><mi>w</mi><mn>1</mn><mi>N</mi></msubsup><mo stretchy=false >)</mo></mrow></mfrac><mi>N</mi></mroot></mrow><annotation encoding="application/x-tex"> \text{Perplexity}(w_1^N) = \sqrt[N]{\frac{1}{P(w_1^N)}} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Perplexity</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.247em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:3.04em;vertical-align:-1.2351em;"></span><span class="mord sqrt"><span class=root ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.6835em;"><span style="top:-2.8419em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8049em;"><span class=svg-align  style="top:-5em;"><span class=pstrut  style="height:5em;"></span><span class=mord  style="padding-left:1em;"><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.3214em;"><span style="top:-2.2869em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8231em;"><span style="top:-2.4337em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2663em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.9794em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.7649em;"><span class=pstrut  style="height:5em;"></span><span class=hide-tail  style="min-width:1.02em;height:3.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='3.08em' viewBox='0 0 400000 3240' preserveAspectRatio='xMinYMin slice'><path d='M473,2793 c339.3,-1799.3,509.3,-2700,510,-2702 l0 -0 c3.3,-7.3,9.3,-11,18,-11 H400000v40H1017.7 s-90.5,478,-276.2,1466c-185.7,988,-279.5,1483,-281.5,1485c-2,6,-10,9,-24,9 c-8,0,-12,-0.7,-12,-2c0,-1.3,-5.3,-32,-16,-92c-50.7,-293.3,-119.7,-693.3,-207,-1200 c0,-1.3,-5.3,8.7,-16,30c-10.7,21.3,-21.3,42.7,-32,64s-16,33,-16,33s-26,-26,-26,-26 s76,-153,76,-153s77,-151,77,-151c0.7,0.7,35.7,202,105,604c67.3,400.7,102,602.7,104, 606zM1001 80h400000v40H1017.7z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.2351em;"><span></span></span></span></span></span></span></span></span></span> <p>Where:</p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> is the number of words in the test set.</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy=false >(</mo><msubsup><mi>w</mi><mn>1</mn><mi>N</mi></msubsup><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> P(w_1^N) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8413em;"><span style="top:-2.4519em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2481em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> is the probability of the test set under the model.</p> </ul> <p>Alternatively, you may see it represented using the log probability:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mtext>Perplexity</mtext><mo stretchy=false >(</mo><msubsup><mi>w</mi><mn>1</mn><mi>N</mi></msubsup><mo stretchy=false >)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence=true >(</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=false >(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant=normal >∣</mi><msubsup><mi>w</mi><mn>1</mn><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy=false >)</mo><mo fence=true >)</mo></mrow></mrow><annotation encoding="application/x-tex"> \text{Perplexity}(w_1^N) = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1^{i-1})\right) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Perplexity</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.247em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:3.106em;vertical-align:-1.2777em;"></span><span class=mop >exp</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class=mord >−</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.3214em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.2777em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8747em;"><span style="top:-2.4436em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2564em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span> <p>This formula calculates the geometric mean of the inverse probability of the words in the test set.</p> <h3 id=application ><a href="#application" class=header-anchor >Application:</a></h3> <p>Sure, here are code examples in Python, using the <code>transformers</code> library by Hugging Face and a simple n-gram model to illustrate each of the scenarios described:</p> <h3 id=language_modeling_evaluation ><a href="#language_modeling_evaluation" class=header-anchor ><ol> <li><p>Language Modeling Evaluation</p> </ol> </a></h3> <p><strong>Example:</strong> Evaluate two language models on a test dataset and compare their perplexities.</p> <pre><code class="python hljs"><span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> GPT2LMHeadModel, GPT2Tokenizer
<span class=hljs-keyword >import</span> torch

<span class=hljs-comment ># Load pre-trained models</span>
model_a = GPT2LMHeadModel.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)
model_b = GPT2LMHeadModel.from_pretrained(<span class=hljs-string >&#x27;gpt2-medium&#x27;</span>)
tokenizer = GPT2Tokenizer.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)

<span class=hljs-comment ># Example test text</span>
test_text = <span class=hljs-string >&quot;This is a test sentence.&quot;</span>

<span class=hljs-comment ># Tokenize input</span>
inputs = tokenizer(test_text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)

<span class=hljs-comment ># Calculate perplexity for Model A</span>
<span class=hljs-keyword >with</span> torch.no_grad():
    outputs_a = model_a(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
    loss_a = outputs_a.loss
    perplexity_a = torch.exp(loss_a)

<span class=hljs-comment ># Calculate perplexity for Model B</span>
<span class=hljs-keyword >with</span> torch.no_grad():
    outputs_b = model_b(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
    loss_b = outputs_b.loss
    perplexity_b = torch.exp(loss_b)

<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Model A Perplexity: <span class=hljs-subst >{perplexity_a.item()}</span>&quot;</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Model B Perplexity: <span class=hljs-subst >{perplexity_b.item()}</span>&quot;</span>)</code></pre> <h3 id=ol_start2_model_comparison ><a href="#ol_start2_model_comparison" class=header-anchor ><ol start=2 > <li><p>Model Comparison</p> </ol> </a></h3> <p><strong>Example:</strong> Compare an n-gram model with a transformer-based model on the same dataset.</p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> nltk
<span class=hljs-keyword >from</span> nltk.lm <span class=hljs-keyword >import</span> MLE
<span class=hljs-keyword >from</span> nltk.lm.preprocessing <span class=hljs-keyword >import</span> padded_everygram_pipeline
<span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> GPT2LMHeadModel, GPT2Tokenizer
<span class=hljs-keyword >import</span> torch

<span class=hljs-comment ># Prepare n-gram model</span>
train_data = [[<span class=hljs-string >&quot;this&quot;</span>, <span class=hljs-string >&quot;is&quot;</span>, <span class=hljs-string >&quot;a&quot;</span>, <span class=hljs-string >&quot;test&quot;</span>, <span class=hljs-string >&quot;sentence&quot;</span>]]
n = <span class=hljs-number >3</span>
train_data, padded_sents = padded_everygram_pipeline(n, train_data)
ngram_model = MLE(n)
ngram_model.fit(train_data, padded_sents)

<span class=hljs-comment ># Example test text</span>
test_text = <span class=hljs-string >&quot;This is a test sentence.&quot;</span>
test_tokens = nltk.word_tokenize(test_text.lower())

<span class=hljs-comment ># Calculate perplexity for n-gram model</span>
ngram_perplexity = ngram_model.perplexity(test_tokens)

<span class=hljs-comment ># Load transformer model</span>
model = GPT2LMHeadModel.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)
tokenizer = GPT2Tokenizer.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)

<span class=hljs-comment ># Tokenize input</span>
inputs = tokenizer(test_text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)

<span class=hljs-comment ># Calculate perplexity for transformer model</span>
<span class=hljs-keyword >with</span> torch.no_grad():
    outputs = model(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
    loss = outputs.loss
    transformer_perplexity = torch.exp(loss)

<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;N-gram Model Perplexity: <span class=hljs-subst >{ngram_perplexity}</span>&quot;</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Transformer Model Perplexity: <span class=hljs-subst >{transformer_perplexity.item()}</span>&quot;</span>)</code></pre> <h3 id=ol_start3_hyperparameter_tuning ><a href="#ol_start3_hyperparameter_tuning" class=header-anchor ><ol start=3 > <li><p>Hyperparameter Tuning</p> </ol> </a></h3> <p><strong>Example:</strong> Tune the number of hidden units in a neural network-based language model and evaluate perplexity.</p> <pre><code class="python hljs"><span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> GPT2Config, GPT2LMHeadModel, GPT2Tokenizer
<span class=hljs-keyword >import</span> torch

<span class=hljs-comment ># Function to create and evaluate model with different hidden units</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">evaluate_model</span>(<span class=hljs-params >hidden_size</span>):
    config = GPT2Config(n_embd=hidden_size)
    model = GPT2LMHeadModel(config)
    tokenizer = GPT2Tokenizer.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)

    <span class=hljs-comment ># Example training data (would normally be more extensive)</span>
    train_text = <span class=hljs-string >&quot;This is a training sentence.&quot;</span>
    inputs = tokenizer(train_text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)
    
    <span class=hljs-comment ># Example test data</span>
    test_text = <span class=hljs-string >&quot;This is a test sentence.&quot;</span>
    test_inputs = tokenizer(test_text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)

    <span class=hljs-comment ># Train the model (dummy training loop for illustration)</span>
    model.train()
    optimizer = torch.optim.Adam(model.parameters())
    optimizer.zero_grad()
    outputs = model(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
    loss = outputs.loss
    loss.backward()
    optimizer.step()

    <span class=hljs-comment ># Evaluate perplexity on test data</span>
    model.<span class=hljs-built_in >eval</span>()
    <span class=hljs-keyword >with</span> torch.no_grad():
        outputs = model(**test_inputs, labels=test_inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
        test_loss = outputs.loss
        perplexity = torch.exp(test_loss)

    <span class=hljs-keyword >return</span> perplexity.item()

<span class=hljs-comment ># Evaluate models with different hidden sizes</span>
hidden_sizes = [<span class=hljs-number >50</span>, <span class=hljs-number >100</span>, <span class=hljs-number >200</span>, <span class=hljs-number >300</span>]
<span class=hljs-keyword >for</span> hidden_size <span class=hljs-keyword >in</span> hidden_sizes:
    perplexity = evaluate_model(hidden_size)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Hidden Size: <span class=hljs-subst >{hidden_size}</span>, Perplexity: <span class=hljs-subst >{perplexity}</span>&quot;</span>)</code></pre> <h3 id=ol_start4_cross-validation ><a href="#ol_start4_cross-validation" class=header-anchor ><ol start=4 > <li><p>Cross-Validation</p> </ol> </a></h3> <p><strong>Example:</strong> Perform cross-validation and calculate average perplexity.</p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> GPT2LMHeadModel, GPT2Tokenizer
<span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >from</span> sklearn.model_selection <span class=hljs-keyword >import</span> KFold

<span class=hljs-comment ># Example dataset</span>
texts = [<span class=hljs-string >&quot;This is the first sentence.&quot;</span>, <span class=hljs-string >&quot;Here is another sentence.&quot;</span>, <span class=hljs-string >&quot;More data for training.&quot;</span>, <span class=hljs-string >&quot;Validation sentence here.&quot;</span>, <span class=hljs-string >&quot;Final sentence for cross-validation.&quot;</span>]

<span class=hljs-comment ># Prepare tokenizer</span>
tokenizer = GPT2Tokenizer.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)

<span class=hljs-comment ># Function to calculate perplexity</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">calculate_perplexity</span>(<span class=hljs-params >model, text</span>):
    inputs = tokenizer(text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)
    <span class=hljs-keyword >with</span> torch.no_grad():
        outputs = model(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
        loss = outputs.loss
        perplexity = torch.exp(loss)
    <span class=hljs-keyword >return</span> perplexity.item()

<span class=hljs-comment ># K-Fold Cross-Validation</span>
kf = KFold(n_splits=<span class=hljs-number >5</span>)
perplexities = []

<span class=hljs-keyword >for</span> train_index, test_index <span class=hljs-keyword >in</span> kf.split(texts):
    train_texts = [texts[i] <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> train_index]
    test_texts = [texts[i] <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> test_index]
    
    <span class=hljs-comment ># Train the model (simple example, normally you&#x27;d use more sophisticated training)</span>
    model = GPT2LMHeadModel.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)
    model.train()
    optimizer = torch.optim.Adam(model.parameters())
    
    <span class=hljs-keyword >for</span> train_text <span class=hljs-keyword >in</span> train_texts:
        inputs = tokenizer(train_text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)
        optimizer.zero_grad()
        outputs = model(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    
    <span class=hljs-comment ># Evaluate the model</span>
    model.<span class=hljs-built_in >eval</span>()
    fold_perplexities = [calculate_perplexity(model, text) <span class=hljs-keyword >for</span> text <span class=hljs-keyword >in</span> test_texts]
    perplexities.extend(fold_perplexities)

<span class=hljs-comment ># Calculate average perplexity</span>
average_perplexity = np.mean(perplexities)
<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Average Perplexity: <span class=hljs-subst >{average_perplexity}</span>&quot;</span>)</code></pre> <h3 id=ol_start5_human_evaluation ><a href="#ol_start5_human_evaluation" class=header-anchor ><ol start=5 > <li><p>Human Evaluation</p> </ol> </a></h3> <p><strong>Example:</strong> Use perplexity as a proxy for evaluating a chatbot model.</p> <pre><code class="python hljs"><span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> GPT2LMHeadModel, GPT2Tokenizer
<span class=hljs-keyword >import</span> torch

<span class=hljs-comment ># Load chatbot model</span>
model = GPT2LMHeadModel.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)
tokenizer = GPT2Tokenizer.from_pretrained(<span class=hljs-string >&#x27;gpt2&#x27;</span>)

<span class=hljs-comment ># Function to calculate perplexity</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">calculate_perplexity</span>(<span class=hljs-params >model, text</span>):
    inputs = tokenizer(text, return_tensors=<span class=hljs-string >&#x27;pt&#x27;</span>)
    <span class=hljs-keyword >with</span> torch.no_grad():
        outputs = model(**inputs, labels=inputs[<span class=hljs-string >&quot;input_ids&quot;</span>])
        loss = outputs.loss
        perplexity = torch.exp(loss)
    <span class=hljs-keyword >return</span> perplexity.item()

<span class=hljs-comment ># Initial evaluation</span>
initial_text = <span class=hljs-string >&quot;Hello, how can I help you today?&quot;</span>
initial_perplexity = calculate_perplexity(model, initial_text)
<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Initial Perplexity: <span class=hljs-subst >{initial_perplexity}</span>&quot;</span>)

<span class=hljs-comment ># After fine-tuning (dummy example)</span>
<span class=hljs-comment ># Fine-tuning code would go here</span>

<span class=hljs-comment ># Evaluate after fine-tuning</span>
fine_tuned_text = <span class=hljs-string >&quot;Hi, I&#x27;m here to assist you. What do you need help with?&quot;</span>
fine_tuned_perplexity = calculate_perplexity(model, fine_tuned_text)
<span class=hljs-built_in >print</span>(<span class=hljs-string >f&quot;Fine-Tuned Perplexity: <span class=hljs-subst >{fine_tuned_perplexity}</span>&quot;</span>)</code></pre> <p>These examples demonstrate how to use perplexity in different scenarios for language model evaluation, model comparison, hyperparameter tuning, cross-validation, and as a proxy for human evaluation. In summary, perplexity is a useful metric for assessing the performance of language models, particularly in the context of predicting sequences of words. It&#39;s widely used in NLP research and applications for model training, evaluation, and comparison.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>