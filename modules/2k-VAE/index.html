<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=variational_automatic_encoders ><a href="#variational_automatic_encoders" class=header-anchor >Variational Automatic Encoders </a></h2> <div class=franklin-toc ><ol><li><a href="#variational_automatic_encoders">Variational Automatic Encoders </a><li><a href="#generative_vs_discriminative_modeling">Generative vs Discriminative Modeling</a><li><a href="#two_popular_generative_modeling_techniques_and_their_comparisons">Two Popular Generative Modeling Techniques and Their Comparisons</a><li><a href="#the_motivation_behind_vae">The Motivation behind VAE</a><li><a href="#parameterizing_a_categorical_distribution">Parameterizing A Categorical Distribution</a><li><a href="#the_use_of_directed_probabilistic_models">The Use of Directed Probabilistic Models</a><li><a href="#minimizing_kl-divergence_or_maximizing_the_elbo_of_parametrized_probabilities">Minimizing KL-Divergence or Maximizing The ELBO of Parametrized Probabilities </a><li><a href="#reconstruct_the_log-likelihood_loss_for_a_factorized_bernoulli_model">Reconstruct The Log-Likelihood Loss for A Factorized Bernoulli Model</a><li><a href="#comparing_optimization_through_minimizing_kl-divergence_vs_sse_in_fitting_an_affine_function_to_data_points">Comparing Optimization through Minimizing KL-Divergence vs SSE in Fitting An Affine Function To Data Points</a><li><a href="#comparing_variational_encoder_with_support_vector_machine">Comparing Variational Encoder with Support Vector Machine </a><li><a href="#comparing_variational_encoder_with_mutual_information">Comparing Variational Encoder with Mutual Information </a></ol></div> <p>Let&#39;s introduce the framework of variational autoencoders &#40;VAEs&#41;, referencing works by Kingma and Welling &#40;2014&#41;, and Rezende et al. &#40;2014&#41;. VAEs are a method for learning deep latent-variable models and inference models using stochastic gradient descent. The framework is applicable to various areas such as generative modeling and semi-supervised learning.</p> <p>We are going to expand on the earlier work done by Kingma and Welling &#40;2014&#41;, focusing on explaining the topic in finer detail and discussing important follow-up work. It is noted that the text is not a comprehensive review of all related work and assumes the reader has basic knowledge of algebra, calculus, and probability theory.</p> <p>The chapter discussed in the excerpt covers background material on probabilistic models, directed graphical models, and the integration of these models with neural networks, specifically in the context of deep latent-variable models &#40;DLVMs&#41;.</p> <h2 id=generative_vs_discriminative_modeling ><a href="#generative_vs_discriminative_modeling" class=header-anchor >Generative vs Discriminative Modeling</a></h2> <p>This section discusses the attractiveness and vulnerabilities of generative modeling and its various applications. Here&#39;s a summary of the key points:</p> <table><tr><th align=center >1. <strong>Expressing Physical Laws and Constraints:</strong><tr><td align=center >Generative modeling allows the incorporation of physical laws and constraints into the modeling process. Nuisance variables, or details that are not critical, are treated as noise.<tr><td align=center >Resulting models are intuitive and interpretable, and by testing them against observations, theories about how the world works can be confirmed or rejected.</table> <table><tr><th align=center >2. <strong>Expressing Causal Relations:</strong><tr><td align=center >Understanding the generative process of data naturally expresses causal relations in the world.<tr><td align=center >Causal relations generalize better to new situations than mere correlations. Knowledge of the generative process can be applied across different scenarios.</table> <table><tr><th align=center >3. <strong>Generative Model to Discriminator:</strong><tr><td align=center >To turn a generative model into a discriminator, Bayes rule is applied. Comparing different generative models can help compute probabilities for events based on observed data.<tr><td align=center >Applying Bayes rule can be computationally expensive.</table> <table><tr><th align=center >4. <strong>Discriminative Methods:</strong><tr><td align=center >Discriminative methods directly learn a mapping for making future predictions.<tr><td align=center >Unlike generative models, discriminative models map input directly to labels. They may lead to fewer errors in discriminative tasks, especially in situations with a large amount of data.</table> <table><tr><th align=center >5. <strong>SemiSupervised Learning:</strong><tr><td align=center >Generative modeling can guide the training of discriminative models, especially in semisupervised learning settings with few labeled examples and many unlabeled examples.</table> <table><tr><th align=center >6. <strong>Generative Modeling as an Auxiliary Task:</strong><tr><td align=center >Generative modeling can serve as an auxiliary task, helping to predict the immediate future and building useful abstractions of the world.<tr><td align=center >This quest for disentangled, semantically meaningful, statistically independent, and causal factors is known as unsupervised representation learning.</table> <table><tr><th align=center >7. <strong>Variational Autoencoder &#40;VAE&#41;:</strong><tr><td align=center >The VAE is highlighted as a tool extensively employed for unsupervised representation learning.<tr><td align=center >It is viewed as an implicit form of regularization, biasing the representation to be meaningful for data generation and improving downstream predictions.</table> <p>Overall, the text emphasizes the versatility of generative modeling in expressing physical laws, understanding causal relations, guiding discriminative models, and serving as an auxiliary task for various applications. The VAE is specifically mentioned as a powerful tool in unsupervised representation learning.</p> <h2 id=two_popular_generative_modeling_techniques_and_their_comparisons ><a href="#two_popular_generative_modeling_techniques_and_their_comparisons" class=header-anchor >Two Popular Generative Modeling Techniques and Their Comparisons</a></h2> <p>This section discusses two popular generative modeling paradigms: Variational Autoencoders &#40;VAEs&#41; and Generative Adversarial Networks &#40;GANs&#41;. VAEs and GANs are seen as having complementary properties. </p> <p>The statement below tries to capture some of the characteristics and tradeoffs between Generative Adversarial Networks &#40;GANs&#41; and Variational Autoencoders &#40;VAEs&#41;:</p> <table><tr><th align=center >1. <strong>GANs:</strong><tr><td align=center ><strong>Strengths:</strong><tr><td align=center >GANs are known for generating high perceptual quality images. The samples generated by GANs often look realistic and can be visually appealing.<tr><td align=center >They leverage a discriminatorgenerator framework, where a generator produces samples to try and fool a discriminator, leading to a competitive and adversarial training process.<tr><td align=center ><strong>Limitations:</strong><tr><td align=center >GANs may lack full support over the data. This means that the distribution they capture might not cover the entire space of possible data points. There could be regions of the data space where the generator struggles to produce realistic samples.</table> <table><tr><th align=center >2. <strong>VAEs:</strong><tr><td align=center ><strong>Strengths:</strong><tr><td align=center >VAEs are likelihoodbased models. They provide a probabilistic framework for generative modeling, making them strong density models.<tr><td align=center >VAEs are better suited for capturing uncertainties in the data and can offer more reliable probability estimates for generated samples.<tr><td align=center ><strong>Limitations:</strong><tr><td align=center >VAEs might generate more dispersed samples. This dispersion refers to the model being less likely to produce highly specific or sharp samples compared to GANs.</table> <p><strong>Tradeoffs and Complementary Nature:</strong> GANs and VAEs often exhibit complementary characteristics. GANs are excellent at capturing highlevel features and generating visually impressive samples, while VAEs provide a more comprehensive probabilistic representation of the data. Researchers have explored hybrid models, like Variational Autoencoder-GANs &#40;VAE-GANs&#41;, to combine the strengths of both approaches, aiming for more realistic and diverse sample generation while maintaining a strong probabilistic foundation.</p> <p>In summary, the choice between GANs and VAEs depends on the specific requirements of the task, with GANs excelling in perceptual quality and VAEs providing a more robust probabilistic framework.</p> <h2 id=the_motivation_behind_vae ><a href="#the_motivation_behind_vae" class=header-anchor >The Motivation behind VAE</a></h2> <p>The passage discusses the role of probabilistic models in machine learning, emphasizing their importance in understanding and predicting natural and artificial phenomena. Probabilistic models are described as mathematical representations that formalize knowledge and skill, serving as central constructs in the field of machine learning and AI.</p> <p>Key points in the passage include:</p> <table><tr><th align=center >1. <strong>Purpose of Probabilistic Models:</strong><tr><td align=center >Probabilistic models are employed to learn mathematical descriptions of phenomena from data.<tr><td align=center >They facilitate understanding, prediction of future unknowns, and various forms of assisted or automated decision-making.</table> <table><tr><th align=center >2. <strong>Incorporating Uncertainty:</strong><tr><td align=center >Due to incomplete data, uncertainty is inherent in probabilistic models.<tr><td align=center >The degree and nature of uncertainty are specified using conditional probability distributions.</table> <table><tr><th align=center >3. <strong>Variable Types:</strong><tr><td align=center >Probabilistic models may involve both continuous and discrete variables.<tr><td align=center >Complete forms of probabilistic models specify all correlations and higher-order dependencies among variables through a joint</table> <table><tr><th align=center >4. <strong>Notation and Observations:</strong><tr><td align=center >The vector <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> represents all observed variables in the model.<tr><td align=center >The observed variable <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> is considered a random sample from an unknown process with an unknown true distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mrow><mtext>|</mtext><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></mrow></msub><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_{\text |{true}}(x))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1052em;vertical-align:-0.3552em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">|</span></span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">e</span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3552em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span>.<tr><td align=center >An approximation <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_{\theta}(x))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span> is chosen, where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span> represents the parameters, to model the underlying process.</table> <table><tr><th align=center >5. <strong>Learning Process:</strong><tr><td align=center >Learning involves finding values for the parameters <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span> that make the model distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_{\theta}(x))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span> closely approximate the true distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mtext>true</mtext></msub><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_{\text{true}}(x))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">true</span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span> for any observed <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span>.</table> <table><tr><th align=center >6. <strong>Flexibility and Incorporating Knowledge:</strong><tr><td align=center >The model <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_{\theta}(x))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span> should be flexible enough to adapt to the data for accurate modeling. It should also allow the incorporation of prior knowledge about the data distribution.</table> <p>Overall, the passage highlights the foundational role of probabilistic models in machine learning, emphasizing their use in capturing and understanding uncertain relationships within data. The learning process involves adjusting model parameters to align the model distribution with the true distribution of the observed data.</p> <h2 id=parameterizing_a_categorical_distribution ><a href="#parameterizing_a_categorical_distribution" class=header-anchor >Parameterizing A Categorical Distribution</a></h2> <p>Parameterizing a categorical distribution refers to expressing the distribution in terms of parameters that define its characteristics. In the context of probability distributions, a categorical distribution is a discrete probability distribution that describes the possible outcomes of a categorical variable, which can take on a finite number of distinct categories.</p> <p>In a categorical distribution, the parameters typically represent the probabilities associated with each category. If there are <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class=mclose >)</span></span></span></span> categories, the categorical distribution would have <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class=mclose >)</span></span></span></span> parameters, each indicating the probability of observing a particular category.</p> <p>Let&#39;s denote the categorical distribution as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>C</mi><mi>a</mi><mi>t</mi><mo stretchy=false >(</mo><msub><mi>p</mi><mn>1</mn></msub><mo separator=true >,</mo><msub><mi>p</mi><mn>2</mn></msub><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(Cat(p_1, p_2, \ldots, p_k))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >))</span></span></span></span>, where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_i)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> represents the probability of the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>i</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(i)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">i</span><span class=mclose >)</span></span></span></span>-th category. The parameters <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mn>1</mn></msub><mo separator=true >,</mo><msub><mi>p</mi><mn>2</mn></msub><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_1, p_2, \ldots, p_k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> are the values that need to be determined or specified to fully define the distribution.</p> <p>For example, if you have a categorical variable representing the outcome of a six-sided die, the categorical distribution would be parameterized by the probabilities of rolling each number from 1 to 6. If the die is fair, the parameters would be <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>=</mo><msub><mi>p</mi><mn>2</mn></msub><mo>=</mo><mo>…</mo><mo>=</mo><msub><mi>p</mi><mn>6</mn></msub><mo>=</mo><mfrac><mn>1</mn><mn>6</mn></mfrac><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_1 = p_2 = \ldots = p_6 = \frac{1}{6})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.3669em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.1901em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8451em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mclose >)</span></span></span></span>.</p> <p>Parameterizing a categorical distribution is essential for various statistical and machine learning applications. The process involves estimating or specifying the values of the parameters based on available data or prior knowledge. Once parameterized, the categorical distribution can be used to model and generate outcomes for the categorical variable.</p> <h2 id=the_use_of_directed_probabilistic_models ><a href="#the_use_of_directed_probabilistic_models" class=header-anchor >The Use of Directed Probabilistic Models</a></h2> <p>The directed probabilistic models, also known as directed probabilistic graphical models &#40;PGMs&#41; or Bayesian networks. </p> <p>These models organize variables into a directed acyclic graph, where the edges indicate probabilistic dependencies. </p> <p>The joint distribution over the variables in such models factorizes into a product of prior and conditional distributions.</p> <p>The mathematical expression for the joint distribution is given by:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mn>1</mn></msub><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><msub><mi mathvariant=bold >x</mi><mi>M</mi></msub><mo stretchy=false >)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mi>p</mi><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mi>j</mi></msub><mi mathvariant=normal >∣</mi><mtext>Pa</mtext><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mi>j</mi></msub><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> p(\mathbf{x}_1, \ldots, \mathbf{x}_M) = \prod_{j=1}^{M} p(\mathbf{x}_j | \text{Pa}(\mathbf{x}_j)) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.4138em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class="mord text"><span class=mord >Pa</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >))</span></span></span></span></span> <p>Here, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mtext>Pa</mtext><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mi>j</mi></msub><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( \text{Pa}(\mathbf{x}_j) )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mopen >(</span><span class="mord text"><span class=mord >Pa</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >))</span></span></span></span> represents the set of parent variables of node <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>j</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(j)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class=mclose >)</span></span></span></span> in the directed graph. For non-root nodes, the distribution conditions on the parents, and for root nodes, the set of parents is empty, resulting in an unconditional distribution.</p> <p>Traditionally, each conditional probability distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mi>j</mi></msub><mi mathvariant=normal >∣</mi><mtext>Pa</mtext><mo stretchy=false >(</mo><msub><mi mathvariant=bold >x</mi><mi>j</mi></msub><mo stretchy=false >)</mo><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( p(\mathbf{x}_j | \text{Pa}(\mathbf{x}_j)) )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class="mord text"><span class=mord >Pa</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathbf">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)))</span></span></span></span> is parameterized using lookup tables or linear models. However, the text suggests a more flexible approach by using neural networks to parameterize these conditional distributions. </p> <p>In this case, neural networks take the parents of a variable as input, allowing for a more expressive and adaptable representation of the conditional probabilities. </p> <p>This utilization of neural networks provides the model with the capacity to capture complex relationships and dependencies within the probabilistic model.</p> <h2 id=minimizing_kl-divergence_or_maximizing_the_elbo_of_parametrized_probabilities ><a href="#minimizing_kl-divergence_or_maximizing_the_elbo_of_parametrized_probabilities" class=header-anchor >Minimizing KL-Divergence or Maximizing The ELBO of Parametrized Probabilities </a></h2> <p>Certainly&#33; Let&#39;s define the mathematical notations for a Bernoulli Variational Autoencoder &#40;VAE&#41;. We&#39;ll use the following notations:</p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>X</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(X)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span></span></span></span>: Input data &#40;binary or multivariate Bernoulli data&#41;</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>Z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(Z)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mclose >)</span></span></span></span>: Latent variable</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span>: Parameters of the model</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>μ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\mu)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">μ</span><span class=mclose >)</span></span></span></span>: Mean of the latent variable distribution</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>log</mi><mo>⁡</mo><mo stretchy=false >(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\log(\sigma^2))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0641em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >))</span></span></span></span>: Log-variance of the latent variable distribution</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>X</mi><mtext>recon</mtext></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(X_{\text{recon}})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">recon</span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span>: Reconstructed data</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >(</mo><mi>X</mi><mi mathvariant=normal >∣</mi><mi>Z</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p(X|Z, \theta))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >))</span></span></span></span>: Likelihood of the data given latent variable and parameters</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >(</mo><mi>Z</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p(Z))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mclose >))</span></span></span></span>: Prior distribution of the latent variable</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>q</mi><mo stretchy=false >(</mo><mi>Z</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(q(Z|X, \theta))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >))</span></span></span></span>: Approximate posterior distribution of the latent variable given data and parameters</p> </ul> <p>The generative process can be expressed as:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>Z</mi><mo>∼</mo><mi>p</mi><mo stretchy=false >(</mo><mi>Z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Z \sim p(Z)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mclose >)</span></span></span></span></span> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>X</mi><mo>∼</mo><mi>p</mi><mo stretchy=false >(</mo><mi>X</mi><mi mathvariant=normal >∣</mi><mi>Z</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">X \sim p(X|Z, \theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span></span> <p>The encoder maps the input data <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>X</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(X)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span></span></span></span> to the distribution over the latent variable <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>Z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(Z)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mclose >)</span></span></span></span>:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>μ</mi><mo separator=true >,</mo><mi>log</mi><mo>⁡</mo><mo stretchy=false >(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=false >)</mo><mo>=</mo><mtext>Encoder</mtext><mo stretchy=false >(</mo><mi>X</mi><mo separator=true >;</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\mu, \log(\sigma^2) = \text{Encoder}(X;\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">μ</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Encoder</span></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span></span> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>Z</mi><mo>∼</mo><mi>q</mi><mo stretchy=false >(</mo><mi>Z</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Z \sim q(Z|X, \theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span></span> <p>The decoder reconstructs the data from the latent variable:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>X</mi><mtext>recon</mtext></msub><mo>=</mo><mtext>Decoder</mtext><mo stretchy=false >(</mo><mi>Z</mi><mo separator=true >;</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">X_{\text{recon}} = \text{Decoder}(Z;\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">recon</span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Decoder</span></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span></span> <p>The loss function consists of two parts: the reconstruction loss and the KL divergence:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>L</mi><mo stretchy=false >(</mo><mi>θ</mi><mo separator=true >,</mo><mi>ϕ</mi><mo separator=true >;</mo><mi>X</mi><mo stretchy=false >)</mo><mo>=</mo><mo>−</mo><msub><mi mathvariant=double-struck >E</mi><mrow><mi>q</mi><mo stretchy=false >(</mo><mi>Z</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow></msub><mo stretchy=false >[</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=false >(</mo><mi>X</mi><mi mathvariant=normal >∣</mi><mi>Z</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo><mo stretchy=false >]</mo><mo>+</mo><mtext>KL</mtext><mo stretchy=false >(</mo><mi>q</mi><mo stretchy=false >(</mo><mi>Z</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo separator=true >,</mo><mi>θ</mi><mo stretchy=false >)</mo><mi mathvariant=normal >∣</mi><mi mathvariant=normal >∣</mi><mi>p</mi><mo stretchy=false >(</mo><mi>Z</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">L(\theta, \phi; X) = -\mathbb{E}_{q(Z|X, \theta)}[\log p(X|Z, \theta)] + \text{KL}(q(Z|X, \theta) || p(Z))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.1052em;vertical-align:-0.3552em;"></span><span class=mord >−</span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">Z</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3552em;"><span></span></span></span></span></span></span><span class=mopen >[</span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)]</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >KL</span></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span><span class=mord >∣∣</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class=mclose >))</span></span></span></span></span> <p>Here, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mtext>KL</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\text{KL})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord text"><span class=mord >KL</span></span><span class=mclose >)</span></span></span></span> denotes the Kullback-Leibler divergence. The first term encourages the model to reconstruct the input faithfully, while the second term regularizes the latent variable distribution to be close to a prior distribution.</p> <p>During training, you would aim to minimize this loss with respect to the model parameters <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\theta)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>ϕ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\phi)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">ϕ</span><span class=mclose >)</span></span></span></span>.</p> <p><strong>An Exmaple in Python Code</strong>:</p> <p>A Deep Latent Variable Model &#40;DLVM&#41; for multivariate Bernoulli data typically involves a generative process that incorporates latent variables to capture the underlying structure of the data. One common example is the Variational Autoencoder &#40;VAE&#41; for binary or multivariate Bernoulli data. Here&#39;s a simplified example using PyTorch, assuming a simple neural network architecture:</p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >import</span> torch.nn <span class=hljs-keyword >as</span> nn
<span class=hljs-keyword >import</span> torch.optim <span class=hljs-keyword >as</span> optim
<span class=hljs-keyword >from</span> torch.distributions <span class=hljs-keyword >import</span> Bernoulli
<span class=hljs-keyword >from</span> torch.nn.functional <span class=hljs-keyword >import</span> binary_cross_entropy_with_logits <span class=hljs-keyword >as</span> bce_loss

<span class=hljs-keyword >class</span> <span class="hljs-title class_">BernoulliVAE</span>(nn.Module):
    <span class=hljs-keyword >def</span> <span class="hljs-title function_">__init__</span>(<span class=hljs-params >self, input_size, hidden_size, latent_size</span>):
        <span class=hljs-built_in >super</span>(BernoulliVAE, <span class="hljs-variable language_">self</span>).__init__()

        <span class=hljs-comment ># Encoder</span>
        <span class="hljs-variable language_">self</span>.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, latent_size * <span class=hljs-number >2</span>)  <span class=hljs-comment ># Two times latent_size for mean and log-variance</span>
        )

        <span class=hljs-comment ># Decoder</span>
        <span class="hljs-variable language_">self</span>.decoder = nn.Sequential(
            nn.Linear(latent_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, input_size),
            nn.Sigmoid()  <span class=hljs-comment ># Sigmoid activation for Bernoulli data</span>
        )

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">reparameterize</span>(<span class=hljs-params >self, mu, log_var</span>):
        std = torch.exp(<span class=hljs-number >0.5</span> * log_var)
        eps = torch.randn_like(std)
        <span class=hljs-keyword >return</span> mu + eps * std

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >self, x</span>):
        <span class=hljs-comment ># Encoder</span>
        enc_output = <span class="hljs-variable language_">self</span>.encoder(x)
        mu, log_var = torch.chunk(enc_output, <span class=hljs-number >2</span>, dim=<span class=hljs-number >1</span>)
        z = <span class="hljs-variable language_">self</span>.reparameterize(mu, log_var)

        <span class=hljs-comment ># Decoder</span>
        recon_x = <span class="hljs-variable language_">self</span>.decoder(z)

        <span class=hljs-keyword >return</span> recon_x, mu, log_var

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">loss_function</span>(<span class=hljs-params >self, recon_x, x, mu, log_var</span>):
        <span class=hljs-comment ># Reconstruction loss (binary cross-entropy)</span>
        recon_loss = bce_loss(recon_x, x, reduction=<span class=hljs-string >&#x27;sum&#x27;</span>)

        <span class=hljs-comment ># KL divergence between the learned latent distribution and the prior</span>
        kl_divergence = -<span class=hljs-number >0.5</span> * torch.<span class=hljs-built_in >sum</span>(<span class=hljs-number >1</span> + log_var - mu.<span class=hljs-built_in >pow</span>(<span class=hljs-number >2</span>) - log_var.exp())

        <span class=hljs-comment ># Total loss</span>
        total_loss = recon_loss + kl_divergence

        <span class=hljs-keyword >return</span> total_loss

<span class=hljs-comment ># Example usage</span>
input_size = <span class=hljs-number >784</span>  <span class=hljs-comment ># Size of input data (e.g., MNIST images)</span>
hidden_size = <span class=hljs-number >256</span>  <span class=hljs-comment ># Size of the hidden layer</span>
latent_size = <span class=hljs-number >32</span>  <span class=hljs-comment ># Size of the latent variable</span>

model = BernoulliVAE(input_size, hidden_size, latent_size)
optimizer = optim.Adam(model.parameters(), lr=<span class=hljs-number >1e-3</span>)

<span class=hljs-comment ># Training loop (assuming you have a dataset loader)</span>
<span class=hljs-keyword >for</span> epoch <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(num_epochs):
    <span class=hljs-keyword >for</span> batch <span class=hljs-keyword >in</span> data_loader:
        data = batch.view(batch.size(<span class=hljs-number >0</span>), -<span class=hljs-number >1</span>)  <span class=hljs-comment ># Flatten the data if needed</span>

        optimizer.zero_grad()
        recon_data, mu, log_var = model(data)
        loss = model.loss_function(recon_data, data, mu, log_var)
        loss.backward()
        optimizer.step()

<span class=hljs-comment ># After training, you can use the decoder to generate new samples</span>
<span class=hljs-keyword >with</span> torch.no_grad():
    z_sample = torch.randn(<span class=hljs-number >1</span>, latent_size)  <span class=hljs-comment ># Sample from the prior</span>
    generated_sample = model.decoder(z_sample)</code></pre> <p>This example demonstrates a simple Bernoulli VAE for modeling binary data, such as images in MNIST. The model consists of an encoder and a decoder, and the training objective includes both reconstruction loss and KL divergence to encourage the learning of meaningful latent representations.</p> <h2 id=reconstruct_the_log-likelihood_loss_for_a_factorized_bernoulli_model ><a href="#reconstruct_the_log-likelihood_loss_for_a_factorized_bernoulli_model" class=header-anchor >Reconstruct The Log-Likelihood Loss for A Factorized Bernoulli Model</a></h2> <p>In the context of a Variational Autoencoder &#40;VAE&#41;, a factorized Bernoulli observation model is a specific choice for the likelihood function that models the distribution of observed data.</p> <p>The Bernoulli distribution is commonly used when dealing with binary data &#40;where each element can take values 0 or 1&#41;. In the case of a factorized Bernoulli observation model, it implies that the observed data <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> is assumed to be generated independently across its dimensions, and each dimension follows a Bernoulli distribution.</p> <p>For a single data point <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> with <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>D</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(D)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=mclose >)</span></span></span></span> dimensions, the probability mass function &#40;PMF&#41; of the factorized Bernoulli distribution is given by:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi>x</mi><mi mathvariant=normal >∣</mi><mi>p</mi><mo stretchy=false >)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><msubsup><mi>p</mi><mi>j</mi><msub><mi>x</mi><mi>j</mi></msub></msubsup><mo>⋅</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub><msup><mo stretchy=false >)</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex"> p(x|p) = \prod_{j=1}^{D} p_j^{x_j} \cdot (1 - p_j)^{1 - x_j} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mord >∣</span><span class="mord mathnormal">p</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.4138em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8435em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2421em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.413em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1.1502em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose ><span class=mclose >)</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> <p>Here, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p_j)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> is the probability of the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>j</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(j)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class=mclose >)</span></span></span></span>-th dimension being 1, and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x_j)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> is the value of the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>j</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(j)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class=mclose >)</span></span></span></span>-th dimension &#40;0 or 1&#41;.</p> <p>In mathematical notation, the log-likelihood of a single data point <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(x)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> under this factorized Bernoulli model can be expressed as:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=false >(</mo><mi>x</mi><mi mathvariant=normal >∣</mi><mi>p</mi><mo stretchy=false >)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><msub><mi>x</mi><mi>j</mi></msub><mi>log</mi><mo>⁡</mo><mo stretchy=false >(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=false >)</mo><mo>+</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=false >)</mo><mi>log</mi><mo>⁡</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \log p(x|p) = \sum_{j=1}^{D} x_j \log(p_j) + (1 - x_j) \log(1 - p_j) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mord >∣</span><span class="mord mathnormal">p</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.4138em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span></span> <p>In the context of a VAE, this expression is often used as the reconstruction loss term <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=false >(</mo><mi>x</mi><mi mathvariant=normal >∣</mi><mi>z</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\log p(x|z))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >))</span></span></span></span>, where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(z)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> is the latent variable associated with the data point. The goal during training is to minimize this log-likelihood loss, encouraging the VAE to generate data points that closely resemble the observed data.</p> <h2 id=comparing_optimization_through_minimizing_kl-divergence_vs_sse_in_fitting_an_affine_function_to_data_points ><a href="#comparing_optimization_through_minimizing_kl-divergence_vs_sse_in_fitting_an_affine_function_to_data_points" class=header-anchor >Comparing Optimization through Minimizing KL-Divergence vs SSE in Fitting An Affine Function To Data Points</a></h2> <p>The process of approximating the posterior distribution in variational inference is conceptually different from fitting an affine function to a set of data points, but there are some similarities in the sense that both involve optimizing parameters to minimize a certain measure of discrepancy.</p> <p>In affine function fitting, the goal is typically to find the parameters &#40;slope and intercept&#41; of a linear function that best fits a given set of data points. This is often done by minimizing the sum of squared errors &#40;SSE&#41; between the observed data points and the predictions of the linear function.</p> <p>In variational inference, on the other hand, the goal is to approximate a complex posterior distribution, such as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi>z</mi><mi mathvariant=normal >∣</mi><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> p(z|x) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mord >∣</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span>, using a simpler distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy=false >(</mo><mi>z</mi><mo separator=true >;</mo><mi>λ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> q(z;\lambda) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">λ</span><span class=mclose >)</span></span></span></span> parameterized by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex"> \lambda </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span>. This is typically done by minimizing the Kullback-Leibler &#40;KL&#41; divergence between <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy=false >(</mo><mi>z</mi><mo separator=true >;</mo><mi>λ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> q(z;\lambda) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">λ</span><span class=mclose >)</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi>z</mi><mi mathvariant=normal >∣</mi><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> p(z|x) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mord >∣</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span>, which measures the discrepancy between the two distributions.</p> <p>While the specific optimization objectives &#40;SSE vs. KL divergence&#41; and parameter spaces &#40;linear function parameters vs. distribution parameters&#41; are different, both tasks involve adjusting parameters to minimize a measure of discrepancy between a model and observed data. Additionally, both tasks often involve iterative optimization algorithms to find the optimal parameters.</p> <p>In summary, while there are similarities in terms of parameter optimization and discrepancy minimization, the objectives and contexts of affine function fitting and variational inference are distinct.</p> <h2 id=comparing_variational_encoder_with_support_vector_machine ><a href="#comparing_variational_encoder_with_support_vector_machine" class=header-anchor >Comparing Variational Encoder with Support Vector Machine </a></h2> <p>Support Vector Machines &#40;SVMs&#41; and variational inference in the context of probabilistic models are conceptually different methods, but they share some similarities in terms of optimization and finding a decision boundary.</p> <table><tr><th align=center >1. <strong>Objective</strong>:<tr><td align=center >SVM aims to find the hyperplane that best separates the classes in the input space, maximizing the margin between the classes. Variational inference, on the other hand, aims to approximate complex posterior distributions using simpler distributions, typically by minimizing the Kullback-Leibler &#40;KL&#41; divergence between them.</table> <table><tr><th align=center >2. <strong>Decision Boundary</strong>:<tr><td align=center >In SVM, the decision boundary is the hyperplane that separates the classes, while in variational inference, the decision boundary is the region where the posterior distribution approximated by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy=false >(</mo><mi>z</mi><mo separator=true >;</mo><mi>λ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> q(z;\lambda) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">λ</span><span class=mclose >)</span></span></span></span> transitions between different modes or clusters in the data.</table> <table><tr><th align=center >3. <strong>Optimization</strong>:<tr><td align=center >Both SVM and variational inference involve optimization algorithms to find the optimal parameters.<tr><td align=center >SVM typically uses techniques like gradient descent or quadratic programming to find the optimal hyperplane parameters, while variational inference often uses optimization algorithms like gradient descent or stochastic gradient descent to optimize the parameters of the approximating distribution.</table> <table><tr><th align=center >4. <strong>Iterative Process</strong>:<tr><td align=center >Both methods often involve iterative processes to converge to the optimal solution. In SVM, this involves iteratively updating the hyperplane parameters to maximize the margin and minimize misclassification.<tr><td align=center >In variational inference, it involves iteratively updating the parameters of the approximating distribution to minimize the KL divergence.</table> <p>In summary, while SVM and variational inference serve different purposes and operate in different contexts, they both involve optimization techniques to find decision boundaries or approximations that best fit the data or achieve the desired objectives.</p> <h2 id=comparing_variational_encoder_with_mutual_information ><a href="#comparing_variational_encoder_with_mutual_information" class=header-anchor >Comparing Variational Encoder with Mutual Information </a></h2> <p>In the context of Variational Autoencoders &#40;VAEs&#41;, there is a connection between the model&#39;s objective and the mutual information between the latent variable <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( z )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> and the observed data <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( x )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span>.</p> <p>The objective function for a VAE involves maximizing the Evidence Lower Bound &#40;ELBO&#41;, which can be decomposed into two components: the reconstruction loss and the KL divergence. The KL divergence term regularizes the distribution of the latent variable by penalizing deviations from a chosen prior distribution, typically a simple distribution like a standard Gaussian.</p> <p>The mutual information between <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( z )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( x )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> can be related to the KL divergence term. In an ideal scenario, where the posterior distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>q</mi><mo stretchy=false >(</mo><mi>z</mi><mi mathvariant=normal >∣</mi><mi>x</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( q(z|x) )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mord >∣</span><span class="mord mathnormal">x</span><span class=mclose >))</span></span></span></span> perfectly matches the prior distribution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( p(z) )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >))</span></span></span></span>, the KL divergence becomes zero, indicating that the information about <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( z )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> provided by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( x )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> is already present in the prior. In other words, the mutual information between <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( z )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( x )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span> is maximized.</p> <p>However, maximizing the mutual information is often a challenging problem, and directly optimizing it can be computationally expensive. The use of the ELBO, with the KL divergence term acting as a regularizer, indirectly encourages the model to learn a representation where relevant information about <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>z</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">( z )</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mclose >)</span></span></span></span> is captured in the latent variable.</p> <p>In summary, while the direct maximization of mutual information is complex, the regularization effect induced by the KL divergence term in the VAE objective indirectly promotes learning a meaningful representation in the latent space that captures relevant information about the observed data.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>