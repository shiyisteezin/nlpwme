<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h1 id=dropout ><a href="#dropout" class=header-anchor >Dropout</a></h1> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#what_is_dropout_in_ml">What is Dropout in ML?</a><li><a href="#interpretation_and_ensemble_interpretation_in_dropout">Interpretation and Ensemble interpretation in Dropout</a><ol><li><a href="#implications_for_interpretability">Implications for Interpretability</a></ol><li><a href="#the_implementation_details_of_dropout">The Implementation Details of Dropout</a></ol></div> <h2 id=what_is_dropout_in_ml ><a href="#what_is_dropout_in_ml" class=header-anchor >What is Dropout in ML?</a></h2> <p>Dropout is a regularization technique commonly used in neural networks during training. The idea behind dropout is to randomly deactivate &#40;or &quot;drop out&quot;&#41; a random set of neurons during each forward and backward pass of the training phase. This involves setting the output of some neurons to zero with a certain probability.</p> <p><strong>Here&#39;s how dropout works</strong>:</p> <p><strong>During Forward Pass</strong>:</p> <p>For each neuron in the layer, dropout randomly sets its output to zero with a specified probability &#40;dropout rate&#41;. This means the neuron is &quot;dropped out&quot; for that particular training iteration. The remaining neurons&#39; outputs are scaled by a factor to account for the dropped-out neurons.</p> <p><strong>During Backward Pass</strong>:</p> <p>Only the active neurons &#40;not dropped out&#41; participate in the backward pass and receive gradients. Gradients are scaled by the same factor used during the forward pass. The key hyperparameter in dropout is the dropout rate, which determines the probability of dropping out a neuron. Typical values for dropout rates range from 0.2 to 0.5.</p> <p><strong>Why Dropout is Important</strong>:</p> <p><strong>Regularization</strong>:</p> <p>Dropout acts as a form of regularization by preventing co-adaptation of hidden units. It helps prevent the network from relying too much on specific neurons and encourages the network to learn more robust and general features.</p> <p><strong>Reduces Overfitting</strong>:</p> <p>By randomly dropping out neurons during training, dropout introduces noise and prevents the model from fitting the training data too closely. This reduces the risk of overfitting and improves the model&#39;s generalization to unseen data.</p> <p><strong>Ensemble Effect</strong>:</p> <p>Dropout can be interpreted as training an ensemble of multiple models with shared weights. Each dropout mask corresponds to a different subnetwork, and the final prediction is the average of the predictions of all these subnetworks. This ensemble effect contributes to improved generalization.</p> <p><strong>Avoids Co-Adaptation</strong>:</p> <p>Dropout prevents neurons from relying too much on specific input neurons. This encourages each neuron to learn more useful features independently of others, avoiding co-adaptation.</p> <p><strong>Handles Covariate Shift</strong>:</p> <p>Dropout can help with covariate shift, where the distribution of input features may change between training and testing. By making the network more robust during training, dropout can improve performance on unseen data.</p> <p>In summary, dropout is an effective regularization technique that helps prevent overfitting, encourages more robust learning, and can lead to improved generalization performance of neural networks.</p> <h2 id=interpretation_and_ensemble_interpretation_in_dropout ><a href="#interpretation_and_ensemble_interpretation_in_dropout" class=header-anchor >Interpretation and Ensemble interpretation in Dropout</a></h2> <p>In the context of dropout in neural networks, &quot;interpretation&quot; and &quot;ensemble interpretation&quot; refer to understanding the impact of dropout during training and its role in creating an ensemble effect.</p> <ol> <li><p><strong>Interpretation of Dropout</strong>:</p> </ol> <ul> <li><p><strong>Regularization Effect</strong>: Dropout is primarily used as a regularization technique during training. It helps prevent overfitting by randomly dropping out neurons, making the network less reliant on specific neurons and features.</p> <li><p><strong>Forcing Redundancy</strong>: By dropping out neurons randomly, dropout forces the network to learn more robust and redundant representations. Neurons cannot rely too heavily on each other, promoting a more distributed learning.</p> <li><p><strong>Noise Injection</strong>: Dropout can be viewed as injecting noise into the learning process. This noise helps prevent the network from memorizing the training data and encourages it to generalize better to unseen data.</p> </ul> <ol start=2 > <li><p><strong>Ensemble Interpretation of Dropout</strong>:</p> </ol> <ul> <li><p><strong>Ensemble Effect</strong>: Dropout introduces an ensemble effect during training. At each training iteration, a different subset of neurons is active, effectively training different subnetworks.</p> <li><p><strong>Multiple Subnetworks</strong>: The dropout technique can be interpreted as training multiple neural networks with shared weights. Each dropout mask corresponds to a different subnetwork, and the final prediction is essentially an average or combination of the predictions from these subnetworks.</p> <li><p><strong>Improved Generalization</strong>: The ensemble effect contributes to improved generalization performance. The network becomes more robust, as it learns to make predictions that are less sensitive to the presence or absence of specific neurons.</p> </ul> <h3 id=implications_for_interpretability ><a href="#implications_for_interpretability" class=header-anchor >Implications for Interpretability</a></h3> <p><strong>Improved Generalization</strong>: The ensemble interpretation suggests that dropout helps the network generalize better to new, unseen data by learning a more robust representation. Diverse Features**: Dropout encourages the learning of diverse features by preventing neurons from co-adapting. This can result in a network that is more capable of handling variations in the input data.</p> <p><strong>Reduced Sensitivity</strong>: The network becomes less sensitive to specific patterns in the training data, leading to a more stable and reliable model.</p> <p><strong>Practical Considerations</strong>:</p> <p><strong>Training Dynamics</strong>: Dropout impacts the training dynamics, and interpreting its effects can provide insights into how the network adapts over time.</p> <p><strong>Dropout Rate</strong>: The dropout rate is a hyperparameter that influences the strength of regularization. Understanding the impact of different dropout rates on the ensemble interpretation can guide model selection.</p> <p>In summary, the interpretation of dropout involves understanding its regularization effect, noise injection, and the ensemble interpretation. Dropout contributes to improved generalization by training multiple subnetworks, each providing a unique perspective on the data. This ensemble interpretation helps create a more robust and reliable neural network.</p> <h2 id=the_implementation_details_of_dropout ><a href="#the_implementation_details_of_dropout" class=header-anchor >The Implementation Details of Dropout</a></h2> <p>The implementation details of dropout during training and testing phases focuses on how to apply dropout to neural network units and how to maintain the means of inputs during training and testing.</p> <ol> <li><p><strong>Decision on Dropout</strong>:</p> </ol> <ul> <li><p>Decide on which units or layers to apply dropout. Typically, dropout is applied to hidden units &#40;neurons&#41; in the fully connected layers, but the decision might vary based on the specific architecture and problem.</p> </ul> <ol start=2 > <li><p><strong>Dropout Probability <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mclose >)</span></span></span></span></strong>:</p> </ol> <ul> <li><p>Choose the dropout probability &#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mclose >)</span></span></span></span>&#41; which represents the probability that a unit is dropped out during training. Common values range from 0.2 to 0.5.</p> </ul> <ol start=3 > <li><p><strong>Bernoulli Variables</strong>:</p> </ol> <ul> <li><p>For each training sample, independently sample as many Bernoulli variables as there are units. Each Bernoulli variable takes a value of 1 with probability <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(1 - p)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mclose >)</span></span></span></span> &#40;indicating the unit is kept&#41; and 0 with probability <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>p</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(p)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">p</span><span class=mclose >)</span></span></span></span> &#40;indicating the unit is dropped&#41;.</p> </ul> <ol start=4 > <li><p><strong>During Training</strong>:</p> </ol> <ul> <li><p>Multiply the activations of the units by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\frac{1}{1 - p})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.3262em;vertical-align:-0.4811em;"></span><span class=mopen >(</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8451em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mclose >)</span></span></span></span> during training. This is known as &quot;inverted dropout.&quot; The purpose is to scale the activations to compensate for the dropout effect and keep the expected value of the activations consistent.</p> </ul> <ol start=5 > <li><p><strong>During Testing</strong>:</p> </ol> <ul> <li><p>During the testing phase, when making predictions on new, unseen data, the network should not apply dropout. The standard way to achieve this is to keep the network untouched during testing. However, since dropout introduces a scaling effect during training, a correction is needed to maintain the means of the inputs during testing.</p> <li><p>The &quot;inverted dropout&quot; approach is to multiply the activations by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(\frac{1}{1 - p})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.3262em;vertical-align:-0.4811em;"></span><span class=mopen >(</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8451em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mclose >)</span></span></span></span> during training. To maintain the means of the inputs during testing, simply use the unscaled activations.</p> </ul> <p><strong>Standard Dropout &#40;During Training&#41;</strong>:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mtext>During Training: activated units</mtext><mo>=</mo><mtext>activated units</mtext><mo>×</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{During Training: } \text{activated units} = \text{activated units} \times \frac{1}{1 - p} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class=mord >During Training: </span></span><span class="mord text"><span class=mord >activated units</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class=mord >activated units</span></span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >×</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:2.2019em;vertical-align:-0.8804em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.3214em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >1</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span><span class="mord mathnormal">p</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> <p><strong>Inverted Dropout &#40;During Testing&#41;</strong>:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mtext>During Testing: activated units</mtext><mo>=</mo><mtext>activated units</mtext></mrow><annotation encoding="application/x-tex">\text{During Testing: } \text{activated units} = \text{activated units} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class=mord >During Testing: </span></span><span class="mord text"><span class=mord >activated units</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord text"><span class=mord >activated units</span></span></span></span></span></span> <p><strong>Purpose of Scaling</strong>:</p> <ul> <li><p>The scaling during training and its absence during testing aim to keep the expected values of the activations consistent between the two phases. This helps ensure that the network adapts appropriately to the dropout regularization during training while making accurate predictions during testing.</p> </ul> <p>In summary, the &quot;inverted dropout&quot; technique is a common and practical way to implement dropout in neural networks, ensuring proper scaling during training and maintaining consistency during testing.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>