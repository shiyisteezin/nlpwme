<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=t5_or_text-to-text_transfer_transformer ><a href="#t5_or_text-to-text_transfer_transformer" class=header-anchor >T5 or Text-To-Text Transfer Transformer</a></h2> <p>T5 &#40;Text-to-Text Transfer Transformer&#41; is a versatile model designed to handle various natural language processing tasks using a unified text-to-text format. Here&#39;s a breakdown of T5 and a simple coding example:</p> <h3 id=t5_breakdown ><a href="#t5_breakdown" class=header-anchor >T5 Breakdown:</a></h3> <ol> <li><p><strong>Text-to-Text Format</strong>:</p> <ul> <li><p>T5 operates on a text-to-text framework, where both inputs and outputs are represented as text. This uniform format allows different tasks to be formulated as text generation or prediction tasks, enabling the model to be trained and applied across a wide range of tasks.</p> </ul> <li><p><strong>Encoder-Decoder Architecture</strong>:</p> <ul> <li><p>T5 consists of a transformer-based encoder-decoder architecture, similar to models like BERT and GPT. The encoder processes the input text, while the decoder generates the output text.</p> </ul> <li><p><strong>Unified Training Objective</strong>:</p> <ul> <li><p>T5 is trained on a diverse set of text-based tasks using a single objective: minimizing the negative log-likelihood of the target text given the input text. This unified training approach allows T5 to learn generalized language representations that can be fine-tuned for specific tasks.</p> </ul> <li><p><strong>Task Agnostic Pre-training</strong>:</p> <ul> <li><p>During pre-training, T5 learns to perform various tasks such as translation, summarization, question answering, and text classification. By training on a mixture of tasks, T5 acquires rich linguistic knowledge that can be transferred to downstream tasks.</p> </ul> <li><p><strong>Dynamic Masking</strong>:</p> <ul> <li><p>T5 uses dynamic masking during pre-training, where different tokens are masked at each training iteration. This helps the model learn robust representations that capture the underlying structure of the text.</p> </ul> </ol> <h3 id=formula_for_training_objective ><a href="#formula_for_training_objective" class=header-anchor >Formula for Training Objective:</a></h3> <p>Given an input text <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex"> X </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> and a target text <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex"> Y </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span>, the objective is to minimize the negative log-likelihood of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex"> Y </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span> given <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex"> X </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>: <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Loss</mtext><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=false >(</mo><mi>Y</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \text{Loss} = -\log P(Y | X) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord text"><span class=mord >Loss</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >−</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span></span></span></span></p> <p>Where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy=false >(</mo><mi>Y</mi><mi mathvariant=normal >∣</mi><mi>X</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> P(Y | X) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span></span></span></span> is the probability of generating <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex"> Y </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span> from <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex"> X </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> according to the T5 model.</p> <h3 id=simple_coding_example ><a href="#simple_coding_example" class=header-anchor >Simple Coding Example:</a></h3> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >import</span> torch.nn <span class=hljs-keyword >as</span> nn
<span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> T5Tokenizer, T5ForConditionalGeneration

<span class=hljs-comment ># Initialize T5 tokenizer and model</span>
tokenizer = T5Tokenizer.from_pretrained(<span class=hljs-string >&quot;t5-small&quot;</span>)
model = T5ForConditionalGeneration.from_pretrained(<span class=hljs-string >&quot;t5-small&quot;</span>)

<span class=hljs-comment ># Example input text</span>
input_text = <span class=hljs-string >&quot;translate English to French: Hello, how are you?&quot;</span>

<span class=hljs-comment ># Tokenize input text</span>
input_ids = tokenizer.encode(input_text, return_tensors=<span class=hljs-string >&quot;pt&quot;</span>)

<span class=hljs-comment ># Generate output text</span>
output_ids = model.generate(input_ids)

<span class=hljs-comment ># Decode output text</span>
output_text = tokenizer.decode(output_ids[<span class=hljs-number >0</span>], skip_special_tokens=<span class=hljs-literal >True</span>)

<span class=hljs-comment ># Print output text</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Translated text:&quot;</span>, output_text)</code></pre> <p>This example demonstrates how to use a pre-trained T5 model for text translation. First, the input text is tokenized using the T5 tokenizer. Then, the model generates the output text based on the input text. Finally, the output text is decoded from token IDs to human-readable text.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>