<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=the_basic_workflow_of_an_etl_pipeline ><a href="#the_basic_workflow_of_an_etl_pipeline" class=header-anchor >The Basic Workflow of an ETL Pipeline </a></h2> <div class=franklin-toc ><ol><li><a href="#the_basic_workflow_of_an_etl_pipeline">The Basic Workflow of an ETL Pipeline </a><li><a href="#the_basic_structure_of_the_etl_pipeline">The Basic Structure of the ETL pipeline </a><li><a href="#the_structure_of_an_etl_job">The Structure of an ETL job </a><li><a href="#passing_configuration_parameters_to_the_etl_job">Passing Configuration Parameters to the ETL Job</a><li><a href="#packaging_etl_job_dependencies">Packaging ETL Job Dependencies</a><li><a href="#running_the_etl_job">Running the ETL Job</a><li><a href="#debugging_spark_jobs_using_start_spark">Debugging Spark Jobs Using start_spark </a><li><a href="#automated_testing">Automated Testing </a><li><a href="#managing_project_dependencies_using_pipenv">Managing Project Dependencies using Pipenv</a><li><a href="#installing_pipenv">Installing Pipenv</a><li><a href="#pipenv_shells">Pipenv Shells</a><li><a href="#automatic_loading_of_environment_variables">Automatic Loading of Environment Variables</a></ol></div> <p>This blog will go through some basics and steps of going through a ETL pipeline project. </p> <p>Below constitutes what we consider to be a &#39;best practices&#39; approach to writing ETL jobs using Apache Spark and its Python &#40;&#39;PySpark&#39;&#41; APIs. This project addresses the following topics:</p> <p>How to structure ETL code in such a way that it can be easily tested and debugged; How to pass configuration parameters to a PySpark job; How to handle dependencies on other modules and packages; and, what constitutes a &#39;meaningful&#39; test for an ETL job.</p> <h2 id=the_basic_structure_of_the_etl_pipeline ><a href="#the_basic_structure_of_the_etl_pipeline" class=header-anchor >The Basic Structure of the ETL pipeline </a></h2> <p><img src="../extras/etl/dir.png" alt=root-dir  /></p> <h2 id=the_structure_of_an_etl_job ><a href="#the_structure_of_an_etl_job" class=header-anchor >The Structure of an ETL job </a></h2> <p>We suggest isolating the &quot;Transformation&quot; step from the &quot;Extract&quot; and &quot;Load&quot; processes into a separate function that accepts input data arguments in the form of DataFrames and returns the converted data as a single DataFrame in order to make debugging and testing easier. </p> <p>The data extraction, data passing to the transformation function, and loading &#40;or writing&#41; the results to their final destination are the tasks covered by the code that surrounds the use of the transformation function in the <code>main&#40;&#41;</code> job function. </p> <p>Testing is made easier because test or mock data may be directly validated and supplied to the transformation function. This is not possible if all ETL code is included in <code>main&#40;&#41;</code> and references sources and destinations of production data.</p> <p>In general, idempotent transformation functions ought to be created. In technical terms, this means that until the input data changes, successive applications of the transformation function should not affect the basic condition of the output data. </p> <p>One of the main benefits of idempotent ETL jobs is their ability to be scheduled to run repeatedly &#40;for example, by use cron to start the spark-submit command above on a predetermined timetable&#41;, eliminating the need to account for possible dependencies on other ETL processes finishing successfully.</p> <h2 id=passing_configuration_parameters_to_the_etl_job ><a href="#passing_configuration_parameters_to_the_etl_job" class=header-anchor >Passing Configuration Parameters to the ETL Job</a></h2> <p>Although it is possible to pass arguments to <code>etl_job.py</code>, as you would for any generic Python module running as a &#39;main&#39; program - by specifying them after the module&#39;s filename and then parsing these command line arguments - this can get very complicated, very quickly, especially when there are lot of parameters &#40;e.g. credentials for multiple databases, table names, SQL snippets, etc.&#41;. </p> <p>This also makes debugging the code from within a Python interpreter extremely complicated, as you don&#39;t have access to the command line arguments that would ordinarily be passed to the code, when calling it from the command line.</p> <p>A much more effective solution is to send Spark a separate file - e.g. using the <code>--files configs/etl_config.json</code> flag with spark-submit - containing the configuration in JSON format, which can be parsed into a Python dictionary in one line of code with <code>json.loads&#40;config_file_contents&#41;</code>. Testing the code from within a Python interactive console session is also greatly simplified, as all one has to do to access configuration parameters for testing, is to copy and paste the contents of the file - e.g.,</p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> json

config = json.loads(<span class=hljs-string >&quot;&quot;&quot;{&quot;field&quot;: &quot;value&quot;}&quot;&quot;&quot;</span>)</code></pre> <p>For the exact details of how the configuration file is located, opened and parsed, please see the <code>start_spark&#40;&#41;</code> function in <code>dependencies/spark.py</code> &#40;also discussed further below&#41;, which in addition to parsing the configuration file sent to Spark &#40;and returning it as a Python dictionary&#41;, also launches the Spark driver program &#40;the application&#41; on the cluster and retrieves the Spark logger at the same time.</p> <h2 id=packaging_etl_job_dependencies ><a href="#packaging_etl_job_dependencies" class=header-anchor >Packaging ETL Job Dependencies</a></h2> <p>In this project, functions that can be used across different ETL jobs are kept in a module called dependencies and referenced in specific job modules using, for example,</p> <pre><code class="python hljs"><span class=hljs-keyword >from</span> dependencies.spark <span class=hljs-keyword >import</span> start_spark</code></pre>
<p>This package, together with any additional dependencies referenced within it, must be copied to each Spark node for all jobs that use dependencies to run. This can be achieved in one of several ways:</p>
<pre><code class="plaintext hljs">1. send all dependencies as a zip archive together 
with the job, using `--py-files` with Spark submit;

2. formally package and upload dependencies to 
somewhere like the PyPI archive (or a private version) 
and then run pip3 install dependencies on each node; or,

3. a combination of manually copying new modules 
(e.g. dependencies) to the Python path of each node 
and using pip3 install for additional dependencies 
(e.g. for requests).</code></pre>
<p>Option &#40;1&#41; is by far the easiest and most flexible approach, so we will make use of this for now. To make this task easier, especially when modules such as dependencies have additional dependencies &#40;e.g. the requests package&#41;, we have provided the build_dependencies.sh bash script for automating the production of packages.zip, given a list of dependencies documented in <code>Pipfile</code> and managed by the <code>pipenv</code> python application &#40;discussed below&#41;.</p>
<h2 id=running_the_etl_job ><a href="#running_the_etl_job" class=header-anchor >Running the ETL Job</a></h2>
<p>Assuming that the &#36;SPARK_HOME environment variable points to your local Spark installation folder, then the ETL job can be run from the project&#39;s root directory using the following command from the terminal,</p>
<pre><code class="python hljs">$SPARK_HOME/<span class=hljs-built_in >bin</span>/spark-submit \
--master local[*] \
--packages <span class=hljs-string >&#x27;com.somesparkjar.dependency:1.0.0&#x27;</span> \
--py-files packages.<span class=hljs-built_in >zip</span> \
--files configs/etl_config.json \
jobs/etl_job.py</code></pre>
<p>Briefly, the options supplied serve the following purposes:</p>
<pre><code class="julia hljs">--master <span class=hljs-keyword >local</span>[*] - the address of the Spark cluster 
to start the job on. If you have a Spark cluster <span class=hljs-keyword >in</span> 
operation (either <span class=hljs-keyword >in</span> single-executor mode locally, 
or something larger <span class=hljs-keyword >in</span> the cloud) and want to send 
the job there, then modify this with the appropriate 
Spark IP - e.g. spark://the-clusters-ip-address:<span class=hljs-number >7077</span>;

--packages &#x27;com.somesparkjar.dependency:<span class=hljs-number >1.0</span><span class=hljs-number >.0</span>,...&#x27; - 
Maven coordinates <span class=hljs-keyword >for</span> any JAR dependencies required 
by the job (e.g. JDBC driver <span class=hljs-keyword >for</span> connecting to a 
relational database);

--files configs/etl_config.json - the (optional) path 
to any config file that may be required by the ETL job;

--py-files packages.zip - archive containing Python 
dependencies (modules) referenced by the job; and,

jobs/etl_job.py - the Python <span class=hljs-keyword >module</span> file containing 
the ETL job to execute</code></pre>
<p>Full details of all possible options can be found here. Note, that we have left some options to be defined within the job &#40;which is actually a Spark application&#41; - e.g. spark.cores.max and spark.executor.memory are defined in the Python script as it is felt that the job should explicitly contain the requests for the required cluster resources.</p>
<h2 id=debugging_spark_jobs_using_start_spark ><a href="#debugging_spark_jobs_using_start_spark" class=header-anchor >Debugging Spark Jobs Using start_spark </a></h2>
<p>It is not practical to test and debug Spark jobs by sending them to a cluster using spark-submit and examining stack traces for clues on what went wrong. A more productive workflow is to use an interactive console session &#40;e.g. IPython&#41; or a debugger &#40;e.g. the pdb package in the Python standard library or the Python debugger in Visual Studio Code&#41;. </p>
<p>In practice, however, it can be hard to test and debug Spark jobs in this way, as they implicitly rely on arguments that are sent to spark-submit, which are not available in a console or debug session.</p>
<p>We wrote the start_spark function - found in dependencies/spark.py - to facilitate the development of Spark jobs that are aware of the context in which they are being executed - i.e. as spark-submit jobs or within an IPython console, etc. </p>
<p>The expected location of the Spark and job configuration parameters required by the job, is contingent on which execution context has been detected. The docstring for start_spark gives the precise details,</p>
<h2 id=automated_testing ><a href="#automated_testing" class=header-anchor >Automated Testing </a></h2>
<p>In order to test with Spark, we use the pyspark Python package, which is bundled with the Spark JARs required to programmatically start-up and tear-down a local Spark instance, on a per-test-suite basis &#40;we recommend using the setUp and tearDown methods in unittest.TestCase to do this once per test-suite&#41;. Note, that using pyspark to run Spark is an alternative way of developing with Spark as opposed to using the PySpark shell or spark-submit.</p>
<p>Given that we have chosen to structure our ETL jobs in such a way as to isolate the &#39;Transformation&#39; step into its own function &#40;see &#39;Structure of an ETL job&#39; above&#41;, we are free to feed it a small slice of &#39;real-world&#39; production data that has been persisted locally - e.g. in tests/test_data or some easily accessible network directory - and check it against known results &#40;e.g. computed manually or interactively within a Python interactive console session&#41;.</p>
<p>To execute the example unit test for this project run,</p>
<pre><code class="python hljs">pipenv run python -m unittest tests/test_*.py</code></pre>
<h2 id=managing_project_dependencies_using_pipenv ><a href="#managing_project_dependencies_using_pipenv" class=header-anchor >Managing Project Dependencies using Pipenv</a></h2>
<p>We use pipenv for managing project dependencies and Python environments &#40;i.e. virtual environments&#41;. All direct packages dependencies &#40;e.g. NumPy may be used in a User Defined Function&#41;, as well as all the packages used during development &#40;e.g. PySpark, flake8 for code linting, IPython for interactive console sessions, etc.&#41;, are described in the Pipfile. Their precise downstream dependencies are described in Pipfile.lock.</p>
<h2 id=installing_pipenv ><a href="#installing_pipenv" class=header-anchor >Installing Pipenv</a></h2>
<p>To get started with Pipenv, first of all download it - assuming that there is a global version of Python available on your system and on the PATH, then this can be achieved by running the following command,</p>
<pre><code class="python hljs">pip3 install pipenv</code></pre>
<p>Pipenv is also available to install from many non-Python package managers. For example, on OS X it can be installed using the Homebrew package manager, with the following terminal command,</p>
<pre><code class="python hljs">brew install pipenv</code></pre>
<p>For more information, including advanced configuration options, see the official pipenv documentation.</p>
<h2 id=pipenv_shells ><a href="#pipenv_shells" class=header-anchor >Pipenv Shells</a></h2>
<p>Prepending pipenv to every command you want to run within the context of your Pipenv-managed virtual environment can get very tedious. This can be avoided by entering into a Pipenv-managed shell,</p>
<pre><code class="python hljs">pipenv shell</code></pre>
<p>This is equivalent to &#39;activating&#39; the virtual environment; any command will now be executed within the virtual environment. Use exit to leave the shell session.</p>
<h2 id=automatic_loading_of_environment_variables ><a href="#automatic_loading_of_environment_variables" class=header-anchor >Automatic Loading of Environment Variables</a></h2>
<p>Pipenv will automatically pick-up and load any environment variables declared in the .env file, located in the package&#39;s root directory. For example, adding,</p>
<pre><code class="python hljs">SPARK_HOME=applications/spark-<span class=hljs-number >2.3</span><span class=hljs-number >.1</span>/<span class=hljs-built_in >bin</span>
DEBUG=<span class=hljs-number >1</span></code></pre>
<p>Will enable access to these variables within any Python program -e.g. via a call to os.environ&#91;&#39;SPARK_HOME&#39;&#93;. Note, that if any security credentials are placed here, then this file must be removed from source control - i.e. add .env to the .gitignore file to prevent potential security risks.</p>
<div class=page-foot >
    <div class=copyright >
      <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a>
       ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    </div>
  </div>
  </div>
    
    
        <script src="/nlpwme/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>