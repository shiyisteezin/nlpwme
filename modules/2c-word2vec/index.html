<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h1 id=the_importance_of_understanding_context_in_natural_language_processing ><a href="#the_importance_of_understanding_context_in_natural_language_processing" class=header-anchor >The Importance of Understanding Context in Natural Language Processing</a></h1> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#references">References</a><li><a href="#what_is_word2vec">What Is Word2Vec?</a><li><a href="#skip-gram_model">Skip-gram Model</a><li><a href="#cbow_or_continuous_bag_of_words_model">CBOW or Continuous Bag of Words Model</a><li><a href="#a_comparison_between_these_two_models">A Comparison Between These Two Models</a></ol></div> <p>The old school of language theories before the whole Connectionism came out was boggled by the difficulty of incorporating context and nuances. Again as we have talked in the previous sections on <a href="https://shiyis.github.io/nlpwme/modules/1-phil-of-mind/">the historical development of NLP</a>, Connectionism emerged as a response to perceived limitations in the traditional computational theory of mind, particularly in its ability to handle context and capture the complexity of cognitive processes. The computational theory of mind, influenced by classical artificial intelligence &#40;AI&#41;, often relied on symbolic representations and rule-based systems.</p> <p>In the subsequent section of this blog, how the later researches in AI have been able to have a break through in terms of solving the problem will be explained.</p> <h3 id=references ><a href="#references" class=header-anchor >References</a></h3> <p><a href="https://arxiv.org/abs/1402.3722">Word2vec Explained: deriving Mikolov et al.&#39;s negative-sampling word-embedding method</a> by Yoav Goldberg and Omer Levy</p> <h3 id=what_is_word2vec ><a href="#what_is_word2vec" class=header-anchor >What Is Word2Vec?</a></h3> <p>As per definition, Word2Vec is a technique in natural language processing &#40;NLP&#41; that represents words as vectors in a continuous vector space, capturing semantic relationships between words. As an alternative to the simpler one hot encoding method, one of the reasons was that it could not accurately capture the similarity between different words as the cosine similarity could. </p> <p>One tool to address the aforementioned issue is Word2vec. It uses a fixed-length vector to represent each word and makes use of these vectors to more clearly show the linkages of analogies and similarity between various words. The Word2vec tool has two models: the continuous bag of words <a href="effective estimate of word representations in vector space">CBOW</a> and the skip-gram &#91;distributed representations of words and phrases and their compositionality&#93;. We will next examine the two models and how they were trained.</p> <p>For the vectors <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >x</mi><mo separator=true >,</mo><mi mathvariant=bold-italic >y</mi><mo>∈</mo><msup><mi mathvariant=double-struck >R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^d</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7335em;vertical-align:-0.1944em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.8491em;"></span><span class=mord ><span class="mord mathbb">R</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span>, the math formula of cosine similarity is below, </p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mfrac><mrow><msup><mi mathvariant=bold-italic >x</mi><mi mathvariant=normal >⊤</mi></msup><mi mathvariant=bold-italic >y</mi></mrow><mrow><mi mathvariant=normal >∥</mi><mi mathvariant=bold-italic >x</mi><mi mathvariant=normal >∥</mi><mi mathvariant=normal >∥</mi><mi mathvariant=bold-italic >y</mi><mi mathvariant=normal >∥</mi></mrow></mfrac><mo>∈</mo><mo stretchy=false >[</mo><mo>−</mo><mn>1</mn><mo separator=true >,</mo><mn>1</mn><mo stretchy=false >]</mo><mi mathvariant=normal >.</mi></mrow><annotation encoding="application/x-tex">\frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\| \|\boldsymbol{y}\|} \in [-1, 1].</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:2.4621em;vertical-align:-0.936em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.5261em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >∥</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mord >∥∥</span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span><span class=mord >∥</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >[</span><span class=mord >−</span><span class=mord >1</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >1</span><span class=mclose >]</span><span class=mord >.</span></span></span></span></span> <p>As we have talked about in a different blog, recent researches strive to represent mental processes in a more distributed way and also be able to capture their compositionality; therefore, two different methods were introduced: Skip-gram and CBOW. The first one is to capture the context within a sentence, and the second one smoothly samples words through an algorithm called sliding window, which we will go into details in subsequent sections. </p> <h3 id=skip-gram_model ><a href="#skip-gram_model" class=header-anchor >Skip-gram Model</a></h3> <p>The Skip-gram model in Word2Vec is a type of word embedding model designed to learn distributed representations &#40;embeddings&#41; for words in a continuous vector space. The primary objective of Skip-gram is to predict the context words given a target word. Let&#39;s break down the key components of the Skip-gram model:</p> <blockquote> <p><strong>Objective Function</strong>: The training objective is to maximize the probability of the context words given the target word. Mathematically, it involves maximizing the conditional probability of the context words given the target word.</p> </blockquote> <blockquote> <p><strong>Input-Output Pairs</strong>: For each occurrence of a word in the training data, the Skip-gram model generates multiple training examples. Each training example consists of a target word and one of its context words. The context words are sampled from a fixed-size window around the target word.</p> </blockquote> <blockquote> <p><strong>Architecture</strong>: The model is typically a neural network with a single hidden layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer contains the word embeddings &#40;continuous vector representations&#41; for each word in the vocabulary.</p> </blockquote> <blockquote> <p><strong>Softmax Activation</strong>: The output layer uses a softmax activation function, which converts the raw output scores into probabilities. These probabilities represent the likelihood of each word being a context word given the target word.</p> </blockquote> <blockquote> <p><strong>Training</strong>: During training, the model adjusts its parameters &#40;word embeddings and weights&#41; to improve the prediction accuracy of context words for each target word. The training process involves backpropagation and gradient descent to minimize the negative log-likelihood of the observed context words.</p> </blockquote> <blockquote> <p><strong>Word Embeddings</strong>: Once trained, the hidden layer&#39;s weights serve as the word embeddings. These embeddings capture semantic relationships between words based on their co-occurrence patterns.</p> </blockquote> <blockquote> <p><strong>Applications</strong>: The learned word embeddings can be used for various natural language processing tasks, such as similarity analysis, language modeling, and as input representations for downstream machine learning tasks.</p> </blockquote> <p>In summary, the Skip-gram model learns word embeddings by training on the task of predicting context words given a target word. It captures the distributional semantics of words, representing them as vectors in a continuous vector space.</p> <h3 id=cbow_or_continuous_bag_of_words_model ><a href="#cbow_or_continuous_bag_of_words_model" class=header-anchor >CBOW or Continuous Bag of Words Model</a></h3> <p>Continuous Bag of Words &#40;CBOW&#41; model in Word2Vec involves a <code>sliding window</code> during its training process. The sliding window is a mechanism used to define the context of a target word.</p> <p>Here&#39;s how it typically works:</p> <blockquote> <p><strong>Context Window</strong>: CBOW considers a fixed-size context window around each target word. This window defines the neighboring words that are used as input to predict the target word.</p> </blockquote> <blockquote> <p><strong>Sliding Window</strong>: The sliding window moves through the training text, and at each position, it considers the words within the window as the context for the current target word.</p> </blockquote> <blockquote> <p><strong>Training Examples</strong>: For each target word in the text, the CBOW model is trained to predict the target word based on the words within its context window.</p> </blockquote> <blockquote> <p><strong>Parameter</strong>: The size of the context window is a parameter that can be set during the training process. It determines how many words on either side of the target word are considered as context.</p> </blockquote> <p>For example, if the context window size is set to 5, the CBOW model will use the five words to the left and five words to the right of the target word as the context for training at each step. </p> <p>This sliding window mechanism allows the model to capture the local context and syntactic information around each word, helping to learn meaningful word embeddings based on the words that tend to co-occur.</p> <p>The skip-gram model assumes that a word can be used to generate the words that surround it in a text sequence. For example, we assume that the text sequence is <code>the</code>, <code>man</code>, <code>loves</code>, <code>his</code>, and <code>son</code>. We use <code>loves</code> as the central target word and set the context window size to 2. As shown below, given the central target word <code>loves</code>, the skip-gram model is concerned with the conditional probability for generating the context words, <code>the</code>, <code>man</code>, <code>his</code> and <code>son</code>, that are within a distance of no more than 2 words, which is,</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><mtext>the</mtext><mo separator=true >,</mo><mtext>man</mtext><mo separator=true >,</mo><mtext>his</mtext><mo separator=true >,</mo><mtext>son</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy=false >)</mo><mi mathvariant=normal >.</mi></mrow><annotation encoding="application/x-tex">\mathbb{P}(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves}).</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class="mord text"><span class="mord textrm">the</span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textrm">man</span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textrm">his</span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textrm">son</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class=mclose >)</span><span class=mord >.</span></span></span></span></span> <p>We assume that, given the central target word, the context words are generated independently of each other. In this case, the formula above can be rewritten as</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><mtext>the</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy=false >)</mo><mo>⋅</mo><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><mtext>man</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy=false >)</mo><mo>⋅</mo><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><mtext>his</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy=false >)</mo><mo>⋅</mo><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><mtext>son</mtext><mo>∣</mo><mtext>loves</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(\textrm{the}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{man}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{his}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{son}\mid\textrm{loves}) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class="mord text"><span class="mord textrm">the</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class="mord text"><span class="mord textrm">man</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class="mord text"><span class="mord textrm">his</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class="mord text"><span class="mord textrm">son</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord textrm">loves</span></span><span class=mclose >)</span></span></span></span></span> <p><img src="https://www.di.ens.fr/~lelarge/skip-gram.svg" alt="alt&#61;skip-gram-demo" /></p> <p>In the skip-gram model, each word is represented as two <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>-dimension vectors, which are used to compute the conditional probability. We assume that the word is indexed as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> in the dictionary, its vector is represented as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >v</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant=double-struck >R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_i\in\mathbb{R}^d</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6891em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.8491em;"></span><span class=mord ><span class="mord mathbb">R</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span> when it is the central target word, and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >u</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant=double-struck >R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{u}_i\in\mathbb{R}^d</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6891em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol">u</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.8491em;"></span><span class=mord ><span class="mord mathbb">R</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span> when it is a context word. Let the central target word <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">w_c</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5806em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and context word <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">w_o</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5806em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> be indexed as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">o</span></span></span></span> respectively in the dictionary. The conditional probability of generating the context word for the given central target word can be obtained by performing a softmax operation on the vector inner product:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><msub><mi>w</mi><mi>o</mi></msub><mo>∣</mo><msub><mi>w</mi><mi>c</mi></msub><mo stretchy=false >)</mo><mo>=</mo><mfrac><mrow><mtext>exp</mtext><mo stretchy=false >(</mo><msubsup><mi mathvariant=bold-italic >u</mi><mi>o</mi><mi mathvariant=normal >⊤</mi></msubsup><msub><mi mathvariant=bold-italic >v</mi><mi>c</mi></msub><mo stretchy=false >)</mo></mrow><mrow><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi mathvariant=script >V</mi></mrow></munder><mtext>exp</mtext><mo stretchy=false >(</mo><msubsup><mi mathvariant=bold-italic >u</mi><mi>i</mi><mi mathvariant=normal >⊤</mi></msubsup><msub><mi mathvariant=bold-italic >v</mi><mi>c</mi></msub><mo stretchy=false >)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbb{P}(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}} \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:2.5741em;vertical-align:-1.048em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.5261em;"><span style="top:-2.2791em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mop ><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1786em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.08222em;">V</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3271em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord text"><span class=mord >exp</span></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol">u</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8309em;"><span style="top:-2.4231em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2769em;"><span></span></span></span></span></span></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord text"><span class=mord >exp</span></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol">u</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8491em;"><span style="top:-2.453em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.247em;"><span></span></span></span></span></span></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.048em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> <p>where vocabulary index set <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=script >V</mi><mo>=</mo><mo stretchy=false >{</mo><mn>0</mn><mo separator=true >,</mo><mn>1</mn><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><mi mathvariant=normal >∣</mi><mi mathvariant=script >V</mi><mi mathvariant=normal >∣</mi><mo>−</mo><mn>1</mn><mo stretchy=false >}</mo></mrow><annotation encoding="application/x-tex">\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.08222em;">V</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >{</span><span class=mord >0</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >1</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >∣</span><span class="mord mathcal" style="margin-right:0.08222em;">V</span><span class=mord >∣</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >1</span><span class=mclose >}</span></span></span></span>. Assume that a text sequence of length <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> is given, where the word at time step <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> is denoted as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{(t)}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.888em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>. Assume that context words are independently generated given center words. When context window size is <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>, the likelihood function of the skip-gram model is the joint probability of generating all the context words given any center word</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><munder><mo>∏</mo><mrow><mo>−</mo><mi>m</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>m</mi><mo separator=true >,</mo><mtext> </mtext><mi>j</mi><mo mathvariant=normal >≠</mo><mn>0</mn></mrow></munder><mi mathvariant=double-struck >P</mi><mo stretchy=false >(</mo><msup><mi>w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mi>j</mi><mo stretchy=false >)</mo></mrow></msup><mo>∣</mo><msup><mi>w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} \mathbb{P}(w^{(t+j)} \mid w^{(t)})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:3.2666em;vertical-align:-1.4382em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.2671em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mrel mtight">≤</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">≤</span><span class="mord mathnormal mtight">m</span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class=mtight > </span></span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class=strut  style="height:0.8889em;vertical-align:-0.1944em;"></span><span class=inner ><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class=fix ></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.4382em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathbb">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.188em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span></span> <p>After the training, for any word in the dictionary with index <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>, we are going to get its two word vector sets <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5944em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >u</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{u}_i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5944em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol">u</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. In applications of natural language processing &#40;NLP&#41;, the central target word vector in the skip-gram model is generally used as the representation vector of a word.</p> <h3 id=a_comparison_between_these_two_models ><a href="#a_comparison_between_these_two_models" class=header-anchor >A Comparison Between These Two Models</a></h3> <p>The Skip-gram model in Word2Vec does not use a fixed sliding window in the same way as the Continuous Bag of Words &#40;CBOW&#41; model. Instead, the Skip-gram model considers each word-context pair in the training data separately.</p> <p>Here&#39;s a brief comparison of the two models:</p> <p><strong>CBOW</strong>:</p> <p>CBOW predicts a target word based on its surrounding context &#40;a fixed-size window of neighboring words&#41;. It sums up the embeddings of the context words and uses them to predict the target word. Skip-gram:</p> <p><strong>Skip-gram</strong>:</p> <p>On the other hand, takes a target word as input and aims to predict the context words within a certain range. It treats each word-context pair in the training data as a separate training example. In the Skip-gram model, there is no fixed sliding window that moves through the text. Instead, each word is considered in isolation, and the model is trained to predict the words that are likely to appear in its context. The context words can be selected from a fixed-size window around the target word, but it&#39;s not constrained by a fixed window during the entire training process.</p> <p>In summary, while <code>CBOW</code> uses a sliding window to define the context for each target word, <code>Skip-gram</code> treats each word-context pair independently without a fixed sliding window. Both of them are important models for building the distributed and compositional attributes of an utterance. </p> <p>For a full implementation of Word2Vec, please check out this <a href="https://github.com/dataflowr/notebooks/blob/master/Module8/08_Word2vec_pytorch_empty.ipynb">notebook</a>.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>