<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h1 id=what_is_modern_natural_language_processing_about ><a href="#what_is_modern_natural_language_processing_about" class=header-anchor >What Is Modern Natural Language Processing About?</a></h1> <p>To answer this major question, it&#39;s important to draw inference from some of the important concepts in psychology. The first one is <em>The Language of Thought Hypothesis</em> and the second one <em>The Representational Theory of Mind</em>. </p> <p>The first one touches on the importance of how our brain has a schema when it comes to producing language of thought, or called <em>Mentalese</em>. The <em>Representational Theory of Mind</em> on the other hand touches on our cognitive abilities to be able to make conscious decisions, solve problems, and reason. </p> <p>Above may be manifested in a natural language as propositional attitudes. The schema goes as,</p> <blockquote> <p>X believes that p iff X believes that S which is a mental representation and p is the actual manifestation of S.</p> </blockquote> <p>That being said, the language that we used to describe our mental experience is only a representation of what in actuality the thing is inside our brain. This representation could bear a semantic property, be it a denotation or a truth-condition.</p> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#what_does_it_mean_to_have_a_mental_language">What Does It Mean To Have A Mental Language?</a><ol><li><a href="#the_manifestation_of_a_mental_representation">The Manifestation of A Mental Representation</a><li><a href="#naturalizing_intentionality_through_hypothesis_formulation_and_concept_acquisition_in_language_learning_and_understanding">Naturalizing Intentionality Through Hypothesis Formulation and Concept Acquisition in Language <em>Learning</em> and <em>Understanding</em></a><li><a href="#on_the_type-token_relation_of_mental_representations">On the Type-Token Relation of Mental Representations</a></ol><li><a href="#how_are_the_above_premises_important_to_establishing_the_framework_of_natural_language_processing">How Are The Above Premises Important to Establishing the Framework of Natural Language Processing?</a><ol><li><a href="#connectionism_and_the_representational_theory_of_mind_continued">Connectionism and The Representational Theory of Mind Continued</a><li><a href="#the_compositionality_of_mental_representations_addressed_in_syntactic_analysis_of_a_natural_language">The Compositionality of Mental Representations Addressed in Syntactic Analysis of a Natural Language</a><ol><li><a href="#a_simple_introduction_to_the_tg_theory_and_gb_law">A simple introduction to the TG Theory and GB Law</a></ol><li><a href="#addressing_the_issues_with_non-autonomy_of_syntax_and_non-compositionality_of_semantics">Addressing The Issues with Non-Autonomy Of Syntax And Non-compositionality of Semantics</a><li><a href="#the_intuitive_imposition_in_semantics">The Intuitive Imposition in Semantics</a><li><a href="#non-overlapping_mapping_mechanism_a_connectionism_approach_to_representing_complex_phrasalsyntactic_structures">Non-overlapping Mapping Mechanism: A Connectionism Approach To Representing Complex Phrasal/Syntactic Structures</a><li><a href="#simple_distributed_representations_such_as_a_simple_mlp_network_to_map_syntactic_structures">Simple Distributed Representations &#40;Such As A Simple MLP Network&#41; To Map Syntactic Structures.</a></ol><li><a href="#statistical_semantics_and_nlp_techniques">Statistical Semantics and NLP Techniques</a><ol><li><a href="#different_kinds_of_text_processing_and_probabilistic_techniques_summarized">Different Kinds of Text Processing and Probabilistic Techniques Summarized</a></ol><li><a href="#extending_to_large_language_models_and_deep_neural_networks_in_the_20th_century">Extending to Large Language Models and Deep Neural Networks in the 20th Century</a><ol><li><a href="#the_encoder-decoder_architecture">The Encoder-Decoder Architecture</a><li><a href="#attention_is_all_you_need">Attention Is All You Need</a><li><a href="#in_a_nutshell">In A Nutshell</a></ol></ol></div> <h3 id=what_does_it_mean_to_have_a_mental_language ><a href="#what_does_it_mean_to_have_a_mental_language" class=header-anchor >What Does It Mean To Have A Mental Language?</a></h3> <p>Continuing the discussion of LOTH or the <em>The Language of Thought Hypothesis</em>, again one can postulate that it&#39;s closely correlated with the mental representation &#40;drawing reference from <em>the Representational Theory of Mind</em> and the current discussions around understanding how the brain actually works; at least w.r.t the folk psychology&#41;.</p> <p>It confirms that <em>Mentalese</em> exists within the scope of research. Aside from the physical aspects &#40;the basic ability to identify, match, and represent the objective and physical reality&#41;, researches of these &quot;mental representations&quot; strive to incorporate subjective and qualitative dimensions, for example, consciousness and propositional attitudes specifically borne out in the form of a natural language as feelings, beliefs, desires, intentions, and fears.</p> <p>The exactness of a mental language or a mental state is yet unknown. However, in folk psychology, these propositional attitudes are a direct reflection and serve a distinctive function to these mental images, in case the locutionary intends that the proposition A* gets assigned to these mental states S.</p> <p>This might be fundamental for inferential reasoning, assuming the importance of how it&#39;s tied to relevant actions. The mental representations S here are put into &quot;boxes&quot; &#40;a &quot;belief box&quot; or a &quot;desire box&quot;&#41;. That being said,</p> <blockquote> <p>Someone who <em>believes</em> S stands in a psychologically important relation to the proposition expressed by S.</p> </blockquote> <p>So, to talk about the actualization of a mental image in such case, the process goes as,</p> <div class=colbox-blue ><h4 id=the_manifestation_of_a_mental_representation ><a href="#the_manifestation_of_a_mental_representation" class=header-anchor >The Manifestation of A Mental Representation</a></h4> <ul> <li><p>First, the occurrence where I have an intention of doing something;</p> <li><p>And then second the enduring state which is reflected as my long standing belief of something.</p> <li><p>With deductive reasoning and inference &#40;I transition from believing* the premises to believing the conclusion&#41; together as a package, the propositional attitudes reflect these mental images which are the direct objects and constitute the scope of a thought process.</p> </ul></div> <p>Natural languages are compositional meaning that complex sentences are formed through simple linguistic expressions. In the same light, a mental language could be compositional and consist of symbols amenable to semantic analysis. This would be introduced as the COMP or <em>The Compositionality of Mental Processes</em>.</p> <p>LOTH claims that mental states typically have constiuent structures. That said, the mental state of intending something bears weight and complexity on its own not including their propositional objects or mental representations.</p> <p>Ancient LOTH proponents used syllogism and propositional logic to analyze the semantics of <em>Mentalese</em>. Current researchers instead use predicate calculus, which was established in the late 18th century and optimized in the 30s. The premise was that,</p> <blockquote> <p>A Mentalese contains primitive words - including predicates, singular terms, and logical connectives - and these words combine to form complex sentences governed by something like the semantics of the predicate calculus.</p> </blockquote> <p>Logical structure only constitutes partly the complex representations, accompanied with other non-sentential formats including graphs, maps, diagrams, etc. They could also be manifested as ideas and imagistic forms. In fact, logic plays no role in such constructions only loosely connected structures did. </p> <p>Schematic mental maps in relation to actual concrete maps best describe these kinds of mental images and representations. Therefore, pluralism was adopted by some to analyze thoughts, including but not limited to those non-sentential formats mentioned above.</p> <p>However illogical these perceptual mental representations are compositional and complex, and they also display <em>systematicity</em>. </p> <p>Though, the mind could postulate an infinite amount of thoughts and displays <em>productivity</em> &#40;which is another important attribute of <em>Mentalese</em>&#41;, but the important ones are qualified or vetted through propositional attitudes that are able to have an effect. Or intentional causation could only come about where there&#39;s an explicit representation.</p> <p>In a nutshell, to discuss in more seriousness, it&#39;s important to distinguish sharply between mental representations and rules governing the manipulation of mental representations. Similarly applicable to deductive reasoning where we often follow rules of deductive inference without explicitly representing the rules. Or we often as native speakers of a language follow its inherent syntax without explicitly stating the rules that govern it.</p> <h4 id=naturalizing_intentionality_through_hypothesis_formulation_and_concept_acquisition_in_language_learning_and_understanding ><a href="#naturalizing_intentionality_through_hypothesis_formulation_and_concept_acquisition_in_language_learning_and_understanding" class=header-anchor >Naturalizing Intentionality Through Hypothesis Formulation and Concept Acquisition in Language <em>Learning</em> and <em>Understanding</em></a></h4> <p>This part of the discussion is very important as it&#39;s pertaining to how we acquire <em>Mentalese</em>.</p> <p>A huge part of the NLP/NLU research is related to naturalizing the intentionality behind a language &#40;discourse analysis, corpus linguistics, formal semantics, syntactic analysis, etc&#41;. This ties back to the understanding of <em>concept acquisition</em>, and the innateness of toddlers to be able to adopt new things into their <em>Mentalese</em>.</p> <p>One approach is through hypothesis formulation in that, </p> <blockquote> <p>Infants are thought to form hypotheses about the world based on their observations and experiences. Through interactions with their environment, infants test these hypotheses and update their understanding of the world accordingly. For example, when an infant observes objects falling to the ground, they may form a hypothesis about the concept of gravity and test it by dropping different objects to see if they fall.</p> </blockquote> <p>The hypothesis formulation and model testing done around the argument has however suffered two logic fallacies: <em>ad infinitum</em> and <em>the pain of circularity</em>, as children could not possibly develop a meta-meta-language to explain a meta-language nor could they have the ability to represent something unless the concept is already known, which is through the required inductive reasoning in a hypothesis formulation about the denotation of <em>Mentalese</em>.</p> <p>Therefore, it bears that there&#39;s no such a thing as concept learning. And the conclusion was that concepts are unlearned in fact they are innate.</p> <p>In such a case, it&#39;s very important to explain what it means by understanding a natural language? The conclusion was that,</p> <blockquote> <p>A Mentalese is a direct medium of the thought process and not the object of interpretation, as we face the conundrum of having to represent it through a meta-language which suffers the logic fallacy of ad infinitum; put differently, &quot;we face the infinite regress of a meta-language&quot;. Therefore, it&#39;s not perceived or interpreted. One cannot say that they understood the mental language the same way as they understood a natural language.</p> </blockquote> <p>similarly to words &#40;lexical items in a natural language&#41;, we can hardly define what a primitive expression&#39;s semantic properties exactly are. </p> <p>Though it&#39;s easy to illustrate the relations between complex expressions formed from these primitive expressions, researchers sought to create paradigms that could well capture the semantic properties of them. </p> <p>One of the strategies was to adopt functionalism &#40;where <em>mind is the functional organization of the brain</em>&#41;. Therefore, the effort of categorizing what exactly it means by saying <code>&quot;X A&#39;s that p iff there is a mental representation S such that X bears A* to S and S means that P&quot;</code> gets broken down into two steps,</p> <pre><code class="plaintext hljs">1. to explain in naturalistically acceptable terms that it is to bear psychological relation A* to mental representation S.

2. to explain in naturalistically acceptable terms that it is for mental representation S to mean that p.</code></pre> <p>Another strategy is by treating these mental processes as type-token relations. </p> <h4 id=on_the_type-token_relation_of_mental_representations ><a href="#on_the_type-token_relation_of_mental_representations" class=header-anchor >On the Type-Token Relation of Mental Representations</a></h4> <blockquote> <p>e and e* are tokens of the same <em>Mentalese</em> type iff R&#40;e,e<em>&#41;. And individuated in neurological terms it becomes that e and e</em> are the tokens of the same primitive Mentalese type iff e and e* are tokens of the same neural type.</p> </blockquote> <p>This is the process of naturalizing intentionality and individuating it through pure neurology. However above schema faces a challenge as it could not address the issue of <em>multiple realizability</em>, as it is defined heterogeneously in the realm of neuroscience, physics, and biology. </p> <p>In lieu of which, a functional definition is carried out and goes as,</p> <blockquote> <p>e and e* are tokens of the same primitive Mentalese type iff e and e* have the same functional role.</p> </blockquote> <p>The below strategy gains popularity as it facilitates the Turing style model treating mental processes as a &quot;computational role&quot; subscribing the &quot;functional role&quot; to Turing style computationalism formalism. </p> <p>And there are two categories that represent such a &quot;functional role&quot;:</p> <p>💭 <em>MOLECULAR</em> vs <em>HOLIST</em>.</p> <p>Molecular theory extracts the canonical or typical relations between symbols through demarcation; on the other hand, the holist is more fine-grained in the sense that every different type is tokened. In other words, no two entities will have the same mental processes, or</p> <blockquote> <p>e and e* belong to the same primitive Mentalese type iff they have the same &#40;canonical if adopted&#41; functional role under MT and iff they have the same total functional role under HT.</p> </blockquote> <p>To explain in simpler terms as I understood it, looking at a natural language say we have the simple term <em>word</em> and <em>words</em> in English serving as a basic unit or lexical item; they would belong to the same primitive type WORD as the same token under MT and different under HT.</p> <p>However it&#39;s concerned under HT that it might have violated the <em>publicity constraint</em> as propositional attitudes are shareable. And in contemporary literature through the inferential judgement that in a natural language, there&#39;s a vague and arbitrary sense of denotation &#40;The English word &quot;cat&quot; denotes cats, but could denote something had the linguistic convention presumes so&#41;, one can also conclude that in a <em>Mentalese</em>, such a case is applicable.</p> <p>Yet, historical literature suggests an alternative view that addresses an distinction that <em>Mentalese</em> is essentially semantically permeated meaning that they are not arbitrary as a natural language is and at least to some extent they have a fixed denotation.</p> <p>This view is confirmed by the fact that different from a natural language, their intrinsic attribute is schematic and typical, meaning that they are types &#40;if one may ask what ensures such a fixed position?&#41; Therefore, CAT in a <em>Mentalese</em> would have denoted cat by its inherent nature and this <em>semantically permeated approach</em> adopts a classificatory scheme that allows the tokens getting categorized adopt semantic values. Finally, the denotation individuation approach goes as,</p> <blockquote> <p>e and e* are the same primitive types iff they bear the same denotation.</p> </blockquote> <p>However, this approach also faces some challenge. First, it violates the original LOTH&#39;s aim to naturalize intentionality in non-representational computational models, as this approach does not take into account the <em>formal-syntactic types manipulated during mental computation</em>.</p> <p>This may be carried out in the FCS or the formal-syntactic concept of computation&#39;s provided explanation of formalism and semantic integrity; which holds that mental computational is sensitive to syntax independent of semantics or mind is practically speaking a <code>&quot;syntactic machine&quot;</code>.</p> <p><em>On top of it, this approach merely prevents the formal-syntactic <em>Mentalese</em> types while executing a reduction in the naturalization of intentionality</em>.</p> <p>In a nutshell, above individuation theories might not be adequate to account for the entirety of LOTH. More investigation is needed.</p> <h3 id=how_are_the_above_premises_important_to_establishing_the_framework_of_natural_language_processing ><a href="#how_are_the_above_premises_important_to_establishing_the_framework_of_natural_language_processing" class=header-anchor >How Are The Above Premises Important to Establishing the Framework of Natural Language Processing?</a></h3> <p>To answer this question, we might need to discuss the important researches done in the realm around the 80s to 90s. As we have mentioned in previous paragraphs of the scientists&#39; attempts to address the issues with representing a <em>Mentalese</em> in the realm of cognitive science.</p> <p>Around the 90s emerging researches under the name <em>Connection Science</em> has sparked a unique route to addressing issues with representation &#40;e.g. distributed vs. localist representations, classical vs. uniquely connectionist representation, type/token vs. part/whole hierarchies&#41;.</p> <p>As we have introduced in previous sections, together RTM &#43; COMP &#43; CTM &#40;The Representational Theory of Mind, The Compositionality of Mental Processes, plus The Computational Theory of Mind&#41; forges the LOTH. And our cognitive inferential systematicities provide the space for the above hypothesis to stand. </p> <p>Here CTM is excluded in the discussion because we want to discuss current research in the field of NLP/NLU/NLG. Only RTM &#43; COMP is included. This is the original spawning of a new school of LOTH or research into mental processes grounded in <code>connectionism</code>. However, we will talk about the importance researches grounded in symbolicism.</p> <h4 id=connectionism_and_the_representational_theory_of_mind_continued ><a href="#connectionism_and_the_representational_theory_of_mind_continued" class=header-anchor >Connectionism and The Representational Theory of Mind Continued</a></h4> <p>In the 1980s, connectionism gained traction in various fields, including natural language processing, as an alternative to the Turing style computational model or the Computational Theory of Mind or the CTM. This break through is the very manifestation of the representational theory of mental processes.</p> <blockquote> <p>the idea is that instead of treating the mental states as having memory access locations with a central processor and a bunch of strings and symbols ready to be manipulated, the connectionist approach treats them as more primitive.</p> </blockquote> <p>That said, under such condition, there are activations of neurons or so called &quot;weight units&quot; distributed and connected across the system, functioning analogously to the real synapses to our brain.</p> <p>These newer and modern systems were built in hope that they could emulate the actual brain activity and could appreciate that</p> <blockquote> <p>brain states are representational, so a good explanatory model should be able to capture that; second, thought processes are productive and systematic, which is manifested in the ability to conduct inferential reasoning.</p> </blockquote> <p>To accept this position, connectionism has to accept RTM &#43; COMP. Eliminativist connectionists might completely reject this position, and on the other hand implementationist connectionism might accept it in order to endorse the Turing style computational model.</p> <p>Some literature later debated around the necessity of commitment to productivity and systematicity of thoughts. Supporters still seek to illuminate the traditional style of computational system.</p> <p>Another stream of objections go against the fact that the productivity and systematicity nature of language of thought had been greatly exaggerated. The last streak of objections posits that one can still create models that reflect the compositionality and the internal structures without adopting the Turing style model.</p> <p>The connectionists contend that instead of a concatenated structure, distributed representations could well capture the integral parts of LOTH. However, doubt was raised around the idea that if we consider concatenative constituency structure is only one of the realizations of the <em>productivity</em> and <em>systematicity</em> of the LOTH, then these schools of neural models would still be considered as part of the classicism.</p> <blockquote> <p>broadly speaking, classicists would appreciate the structural representations while the connectionists would appreciate the more distributed representations. </p> </blockquote> <p>Connectionist models, based on neural networks, nonetheless offered an alternative to traditional symbolic or computational models, such as those associated with the Computational Theory of Mind &#40;CTM&#41;. These connectionist models were often seen as more biologically plausible and capable of capturing complex, distributed patterns of information processing.</p> <h4 id=the_compositionality_of_mental_representations_addressed_in_syntactic_analysis_of_a_natural_language ><a href="#the_compositionality_of_mental_representations_addressed_in_syntactic_analysis_of_a_natural_language" class=header-anchor >The Compositionality of Mental Representations Addressed in Syntactic Analysis of a Natural Language</a></h4> <p>In this section and the subsequent sections, I will go through the Chomskyan Transformation Grammar Theory to demonstrate how researchers address compositionality through symbolicism. The importance is that natural language modeling and understanding tasks often want to seek solutions that extract the underlying complex syntactic structure of a natural language. This is associated with the fact that natural languages display compositionality similar to a <em>Mentalese</em>.</p> <p>The Schema of a Chomsky hierarchy is demonstrated below,</p> <div class=colbox-blue ><h5 id=a_simple_introduction_to_the_tg_theory_and_gb_law ><a href="#a_simple_introduction_to_the_tg_theory_and_gb_law" class=header-anchor >A simple introduction to the TG Theory and GB Law</a></h5> <p>Formally, a content-free phrase-structure grammar is a 4-tuple <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=false >(</mo><mi>N</mi><mo separator=true >,</mo><mi mathvariant=normal >Σ</mi><mo separator=true >,</mo><mi>P</mi><mo separator=true >,</mo><mi>S</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">G=(N, \Sigma, P, S)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal">G</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >Σ</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class=mclose >)</span></span></span></span>, where:</p> <ul> <li><p>&#40; N and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class=mord >Σ</span></span></span></span> &#41; are finite, disjoint alphabets of non-terminals and terminals respectively.</p> <li><p>P is a finite set of productions of the form <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi mathvariant=normal >B</mi></mrow><annotation encoding="application/x-tex">A \rightarrow \Beta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathrm">B</span></span></span></span> where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">A \in N</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >B</mi><mo>∈</mo><mo stretchy=false >(</mo><mi>N</mi><mo>∪</mo><mi>σ</mi><mo stretchy=false >)</mo><mo>∗</mo></mrow><annotation encoding="application/x-tex">\Beta \in (N \cup \sigma)*</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathrm">B</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >∪</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=mclose >)</span><span class=mord >∗</span></span></span></span>.</p> <li><p>S is a distinguished symbol in N, known as the start symbol.</p> <li><p>For stochastic grammars, each of the productions has an associated probability, with the constraint that the probabilities of all the RHSs for any given LHSs must sum to one.</p> </ul></div> <p>It is often useful to allow more than one start symbol, perhaps one for each pattern class. The set of all possible strings derivable &#40;denoted by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mo>⇒</mo><mo lspace=0em  rspace=0em >∗</mo></msup></mrow><annotation encoding="application/x-tex">\Rightarrow^{*}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6887em;"></span><span class=mrel ><span class=mrel >⇒</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>&#41; from S is denoted <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy=false >(</mo><mi>G</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">L(G)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class=mopen >(</span><span class="mord mathnormal">G</span><span class=mclose >)</span></span></span></span>, i.e. the language generated by <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal">G</span></span></span></span>, where a non-terminal can be rewritten in more than one way, we use the disjunction symbol &#39;|&#39;, e.g. <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi>B</mi><mi>C</mi><mi mathvariant=normal >∣</mi><mi>C</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">A \rightarrow BC|CD</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">BC</span><span class=mord >∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> means <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi>B</mi><mi>C</mi><mo separator=true >,</mo><mi>A</mi><mo>→</mo><mi>C</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">A \rightarrow BC, A \rightarrow CD</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">BC</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span>.</p> <p><em>Deep vs. Surface Phrasal Structure</em> - below is an example of a &quot;passive transformation&quot; changing a phrase&#39;s deep structure by moving the phrase&#39;s noun phrase from object position to subject position. For example,</p> <blockquote> <p>a. d-structure: &#91;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>N</mi><mi>P</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{NP}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4783em;vertical-align:-0.15em;"></span><span class=mord ><span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">NP</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>&#93; INFL see-en BILL</p> </blockquote> <blockquote> <p>b. s-structure: Bill INFL seen-en e</p> </blockquote> <p>Here the letter <em>e</em> represents the empty category that fulfills the empty category principle constraint</p> <p><em>Government Binding</em> - essentially what we see in above example is that in GB there are two phases in generation:</p> <pre><code class="julia hljs"><span class=hljs-number >1.</span> will still require the phrase structure rules. 
<span class=hljs-number >2.</span> the change from d-structure to s-structure.</code></pre> <p><em>The Empty Category Principle</em> - which is the constraint that whenever there&#39;s an empty category left behind by some sort of motion forced by case-filtering needs to be properly governed.</p> <p><em>D-Structure</em> - the d-structure is the product of phrasal structural rules that are derived from X-bar syntax, which provides a template for the structure of phrases &#40;NP, VP, etc&#41;.</p> <p><em>Case Filtering</em> - Case is a grammatical marking which is assigned to phrase with specific structural relationships to various lexical items, such as verbs.</p> <p><em>The <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-Role</em> - similar constraint is the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-criterion. In addition to case, a verb assigns some <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-roles &#40;&#39;thematic roles&#39; such as agent, patient, goal&#41;. The <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-criterion says that a chain must have one and only one <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-role attached to it.</p> <p><em>Movement-<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></em> - During transformation, the d-structure turns into the s-structure which reflects &#40;more or less&#41; the actual form of the generated sentence. This movement is constrained by restrictions on the positions, items moved, and other properties of a chain. This is very crucial to pay attention to if a language modeling task needs to done addressing such a movement.</p> <p>ℹ️ Why is this important to bring up here?</p> <p>Transformational grammar posits that the structure of sentences in natural language can be generated through a series of transformations applied to underlying structures. These transformations allow for the derivation of various surface forms from a single underlying representation, providing a systematic way to account for the diversity of sentences in a language.</p> <p>Compositionality, on the other hand, refers to the principle that the meaning of complex expressions is determined by the meanings of their constituent parts and the rules used to combine them. In the context of natural language, compositionality suggests that the meaning of a sentence can be derived from the meanings of its individual words and the syntactic rules that govern their combination.</p> <h5 id=addressing_the_issues_with_non-autonomy_of_syntax_and_non-compositionality_of_semantics ><a href="#addressing_the_issues_with_non-autonomy_of_syntax_and_non-compositionality_of_semantics" class=header-anchor >Addressing The Issues with Non-Autonomy Of Syntax And Non-compositionality of Semantics</a></h5> <p>The grammar of a language is conceived by cognitive linguistics to be a pair of utterances and their meanings, where &#39;meaning&#39; is broadly conceived to encompass all evoked conceptualization, including communicative function and extralinguistic aspects of the speech act.</p> <p><em>Form-meaning pairs</em> &#40;also called &#39;grammatical constructions&#39; or &#39;constructional schemas&#39;&#41; can be highly specific, as happens with <code>idiosyncratic expressions</code> whose conditions of form and usage must be learned on a case-by-case basis, or they can be schematizations over large numbers of individual pairings.</p> <p>These constructional schemas are the fundamental descriptive and explanatory device in cognitive linguistics; discovering the principles which constrain regularities in the <em>form-meaning pairs</em> of a language is understood to be the goal of linguistic research.</p> <p>Under this interpretation of a grammar, all grammatical forms have a conceptual basis, although often a highly <em>abstract one</em>. There is thus no clear separation between grammar, semantics, and pragmatics, since semantic and pragmatic structures are included in the constructional schemas.</p> <p>One typical example will be to use &quot;peripheral constructions&quot; to cover idiosyncratic expressions like idioms and conventionalized expressions.</p> <h5 id=the_intuitive_imposition_in_semantics ><a href="#the_intuitive_imposition_in_semantics" class=header-anchor >The Intuitive Imposition in Semantics</a></h5> <p>On the other hand, the non-compositionality of semantics indicate that the meaning of an utterance is the mental conceptualization it evokes.</p> <p>Cognitive linguists emphasize the search for principles which will facilitate an explanation of the range of <em>form-meaning pairs</em> in a given language.</p> <p>ℹ️ Why does a specific combinations of words evoke one particular schematic conception rather than another? Why are some word-combinations regarded as ill-formed? How can we describe this evoked conceptualization in a useful manner?</p> <p>Traditional attempts to understand the meaning of natural language utterances have typically divided the problem into two parts:</p> <pre><code class="julia hljs">(<span class=hljs-number >1</span>) how can the meanings of words be characterized.
(<span class=hljs-number >2</span>) how are word meanings combined to yield sentence meaning.</code></pre> <p>The meaning of a word is a set of semantic features used to determine its real-world referent, and sentence meaning is an aggregation of these meanings;the meaning of <em>bachelor</em> would be equated with features such as &#91;HUMAN, MALE. UNMARRIED&#93;. This view has been beneficial in linguistic and psychological research, but has faced challenges such as identifying prime features for concrete nouns and some verbs, and polysemy.</p> <p>Researchers have proposed viewing the composite parts of a word as rules, which can be fluid attributes that shift according to context. Cognitive categorization data in psychology suggests that the conditions on word meanings should be understood as &#39;<strong>preferences rules</strong>&#39;, operating according to constraints satisfaction and violation. While cognition is central to semantics, this approach is limited in that it&#39;s ignoring sentence-level integration.</p> <p>💭 Two of the researches will be introduced to dive a little deeper in this part of the conversation. TL;DR here, one utilizes mapping mechanism to try to capture Chomskyan phenomenon and another uses a simple neural network back-propagation architecture to try to model the conceptual activation in the preposition &#39;<em>over</em>&#39;.</p> <h5 id=non-overlapping_mapping_mechanism_a_connectionism_approach_to_representing_complex_phrasalsyntactic_structures ><a href="#non-overlapping_mapping_mechanism_a_connectionism_approach_to_representing_complex_phrasalsyntactic_structures" class=header-anchor >Non-overlapping Mapping Mechanism: A Connectionism Approach To Representing Complex Phrasal/Syntactic Structures</a></h5> <p>This is another important attempt at bridging these two different schools of LOTH theory development together - the non-overlapping mapping mechanism &#40;Rager and Berg, 1992&#41;. At a simpler level, a breath-first representation as a mapping was provided to capture the d-structure of a phrasal structure to a row and column representation with constituent parts highlighted &#40;where the constituents preceding a constituent form the elements of the column above it as demonstrated in the graph above&#41;. This mapping also needs to satisfy the above constraints, which we will go into more depth in the subsequent sections.</p> <p>A breakdown of such a structure in depth goes as,</p> <div class=colbox-blue ><p><em>Using Maps to Model Constraints on Move-<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></em> Maps are either used for input and output or to enforce the structural constraints on the sentences. The input to the system consists of several rows representing different features of the constituents &#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-marking, case-marking, VP, NP, lexical-NP and maximal projection&#41; and the structure map.</p> <p>The structure map shows, for each constituent in the sentence, which other constituent is its parent &#40;i.e. immediately dominates it&#41; in the structure. This is indicated by activating the unit representing the parent in the column above the constituent in the map. On the left in above figure is the tree representation of the d-structure of the sentence &quot;Bill was seen&quot;.</p> <p>The map in this figure is the structure map, which is equivalent to the d-structure tree. The row is the case-marking row. Active units are indicated by the heavier lines.</p> <p>The output of the system is the <em>chain map</em>. The chain map shows the chains created &#40;if any&#41; by movements in the transition from d-structure to s-structure. If a constituent began a chain, there will be an activated unit in the column above it to indicate the constituent which is the new location after the movement.</p> <p>If there were intermediate steps in the movement, multiple units will be active in the column. From this map and the structure map, the s-structure of the sentence can be determined. The chain map of the passive movement from sentence &#40;1&#41; is given in blow figure. The map tells us that the children of the second NP are moved to be the children of the first NP, while a trace is left behind.</p></div> <p>Most of the maps in the system are not used to represent sentence structure; rather, they are used to enforce the constraints on movement. They do this through their excitatory and inhibitory links to other maps.</p> <p>The remainder of this section describes one of the use of maps to enforce the four constraints: the non-overlap constraint, the case filter, the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>-role, and the empty category principle.</p> <table><tr><th align=center >The non-overlap constraint<tr><td align=center >Says that chains may not overlap. Every unit in the chain map has an excitatory link to the corresponding unit in the non-overlap map. &#40;This type of linking between the units in two maps is called lateral linking&#41;.<tr><td align=center ><tr><td align=center >Thus, the units active in the chain map will activate their counterparts in the non-overlap map. The units in the non-overlap map will in turn have inhibitory diagonal, non-lateral links to their counterparts in the chain map.<tr><td align=center ><tr><td align=center >In diagonal linking every unit in the source map is linked to its counter part in each column of the target &#40;i.e. to the corresponding diagonal&#41;.<tr><td align=center ><tr><td align=center >A diagonal, non-lateral connection is the same as a diagonal connection except that the link to the directly corresponding unit is absent.<tr><td align=center ><tr><td align=center >The connections from an active unit in the non-overlap map to the chain map prevent the activation of any other node in the corresponding diagonal in the chain map, enforcing the constraint. &#40;Nodes in the same diagonal represent the same syntactic marker and if two units both get activated and had moved to the same place, it will violate the non-overlap constraint&#41;.</table> <table><tr><th align=center >chain map with activated units on the right and d-structure on the left<tr><td align=center ><img src="../extras/connectionism/structure-map.jpg" alt="" /></table> <table><tr><th align=center >non-overlap constraints satisfactory map<tr><td align=center ><img src="../extras/connectionism/case-filter-map-annotated.jpg" alt="" /></table> <p>The resulting model is highly complex. However, the implementation and evaluation decision is far from arbitrary. In fact, it&#39;s highly regular and generalized to be applied to other example sentences &#40;though not tested on wh-questions&#41;.</p> <p>In the discussion section, it was noted that AI connectionist approaches seldom think of syntax as being constraint-based, they describe it in terms of high-level rules.</p> <p>However, GB theory imposes constraints globally on resulting structures and can provide insight into ways to model syntax in connectionist systems. Therefore, this research serves as a steppingstone to more interesting approaches and discoveries.</p> <p>In fact, a few papers accomplished in the 1980s put effort into addressing this RTM &#43; COMP through coding the syntactic elements of the inputs at a higher level with a simple MLP neural network to train such a model and accomplish subsequent lower level parsing tasks.</p> <h5 id=simple_distributed_representations_such_as_a_simple_mlp_network_to_map_syntactic_structures ><a href="#simple_distributed_representations_such_as_a_simple_mlp_network_to_map_syntactic_structures" class=header-anchor >Simple Distributed Representations &#40;Such As A Simple MLP Network&#41; To Map Syntactic Structures.</a></h5> <p>This section will introduce one of the many researches that has put in such an endeavour to dive deeper and make the connection happen. In the paper that has conducted <em>Hybrid Noun Phrase Analysis</em>, under section 3 <em>Learning Semantic Prepositional Relationships in Distributed Connnectionist Networks</em>, the net work was built as such that for each preposition there is one back-propagation network that determines the plausibility of the prepositional relationships &#40;see below figure&#41;.</p> <p>One network consists of three layers of units to cover the schema &#39;Sun &#40;Trajector&#41; rises-<em>over</em> &#40;Locative-PREPOSITION&#41; the sea &#40;Landmark&#41;&#39;. The input layer consists of 32 binary units &#40;value 0 and 1; and probably the earliest vectorization of words or word2vec&#41; representing 15 features for each of the two nouns.</p> <p>The single real-valued output unit determines whether the prepositional relationship is plausible &#40;value 1&#41; or implausible &#40;value 0&#41;. Twelve real-valued hidden units encode the mapping from the input units to the output units from a training set. All levels in the back-propagation network are fully connected. The graph of such a simple network is demonstrated below.</p> <p><img src="../extras/connectionism/a-simple-bp.png" alt="" /></p> <p>Such a network was introduced to cover the conceptual experience and idiosyncrasy raised in prepositions, here the example used is OVER as in &#39;<em>sun rises over the ocean</em>&#39;. Because of the polysemous nature of the preposition, through some visualized investigation, it was decided to assign three main categories of spacial relationships for <em>over</em>: the simple &#39;above&#39;, the &#39;above-cross&#39;, and the &#39;cover&#39; relationship.</p> <p>Inferring the intended relationship from a give <em>over</em> expression appears to involve constraint satisfaction: not all the component words in the utterance are compatible with all the major spatial meanings, and the interaction of these constraints typically results in the selection of a single meaning.</p> <p>The second reason presented by the paper was as described above that to model the polysemies of <em>over</em> is to explore the parallels between the &#39;linguist&#39;s task of rule induction and the input-output mappings that back propagation networks can solve. This was one of the pioneer models built around the idea of using a connectionist approach to solve more structured syntactic problems.</p> <p><img src="../extras/connectionism/back-prop-over.png" alt="" /></p> <p>The architecture of the network could be broken down as such,</p> <div class=colbox-blue ><p>The network was given the task of mapping input patterns of the form &quot;trajector verb&#40;over&#41; landmark&quot; as we have already demonstrated in previous sections. The above figure shows the entirety, including # layers, connections between layers, and the contents of the input and output layers.</p> <p><em>The output layer</em>. The output layer consists of six units, one for each of the three schemas, and three to indicate whether the landmark and trajector make &#39;contact&#39;, meaning whether if the path has an &#39;end-point&#39; or &#39;upward&#39; component.</p> <p><em>The input layer</em>. The input layer is reduced to only cover the necessary schema for the network, so a vocab of 18 trajectors, 15 verbs, and 15 landmarks. Also, notably no semantic information was given to the model of the sentence that could be constructed under the schema as a whole &#40;represented in a localist fashion; means encoding each individual piece of information as a distinct and separate unit, rather than embedding it within a larger context or structure. </p> <p>In the context of neural networks, this often involves representing each element of a set &#40;such as words in a vocabulary or components of a schema&#41; as its own distinct vector or activation&#41;. That being said, they were preprocessed as separate units &#40;activated in the vector&#41; in hope that the model could capture some independent inference.</p></div> <p>The details introduced here is very important, as it&#39;s required to discuss how the model learning the patterns gets postulated. In the paper, it was described that in order for the model to use the same set of weighted connections to map a large number of input patterns to their target outputs, then the network would have to learn, from the distributional regularities in the mapping between trajector-verb-landmark combinations and their output features, that</p> <blockquote> <p>some input items are similar to others in some contexts but not in other contexts.</p> </blockquote> <p>For example, some hidden units might learn to adapt to the similarities between a <em>plane</em> and a <em>car</em>, and the differences between a <em>plane</em> and a <em>person</em>.</p> <p>This architecture, in conclusion, had a relatively simple makeup in hope that the function of these hidden units &#40;self-regulate, self-organize, and feature-detect&#41; could be apparent, as they serve a specific functional role in the mapping from input to output layers &#40;i.e. trajector role-specific hidden units will only receive input activated from the trajector vector&#41;.</p> <p>It was done in this style in hope that some inherent properties of the units in these schemas could be well captured.</p> <h3 id=statistical_semantics_and_nlp_techniques ><a href="#statistical_semantics_and_nlp_techniques" class=header-anchor >Statistical Semantics and NLP Techniques</a></h3> <p>Beginning at the end of the 1980s and continuing until the end of 1995, almost every aspect of natural language processing underwent a major transformation: the transition to statistical, corpus-based approaches &#40;as indicated by two special issues on this topic published in the quarterly Computational Linguistics in 1993&#41;. This shift was made possible by the growing availability and volume of machine-readable text and speech data.</p> <p>Statistical Natural Language Processing &#40;NLP&#41; challenges traditional computational linguistics views, revealing that ambiguities are too numerous and interrelated to be resolved by heuristic arbitration. Language phenomena need to be treated as stochastic, and distributional properties systematically exploited for reliable hypotheses.</p> <p>This shift suggests that language and thinking are not only symbolic but also quantitative and probabilistic.</p> <p>In all cases, the three main requirements below are the development of probabilistic models &#40;aided by learning&#41; to relate linguistic inputs to desired outputs, and the use of algorithms to assign labels or structures to previously invisible model inputs.</p> <div class=colbox-blue ><ol> <li><p><strong>Modeling</strong>:</p> <ul> <li><p><strong>Generative vs. Discriminative Models</strong>: These models differ in how they learn and generate data. Generative models aim to understand the underlying distribution of the data, while discriminative models focus on learning the boundary between classes.</p> <li><p><strong>Parametric vs. Non-Parametric Models</strong>: Parametric models make assumptions about the form of the underlying distribution and have a fixed number of parameters, whereas non-parametric models adapt their complexity based on the amount of training data.</p> </ul> <li><p><strong>Learning from Data</strong>:</p> <ul> <li><p><strong>Maximum Likelihood Estimation</strong>: A method used to estimate the parameters of a statistical model by maximizing the likelihood function.</p> <li><p><strong>Maximum Entropy and Expectation Maximization</strong>: Techniques used for modeling complex data distributions and iteratively refining model parameters, respectively.</p> <li><p><strong>Supervised versus Unsupervised Learning</strong>: In supervised learning, models are trained on labeled data, while unsupervised learning involves discovering patterns or structures in unlabeled data.</p> </ul> <li><p><strong>Output Computation</strong>:</p> <ul> <li><p><strong>Dynamic Programming</strong>: A method for efficiently solving problems by breaking them down into simpler subproblems.</p> <li><p><strong>Unique Output vs. Distribution of Output</strong>: This refers to whether the model generates a single output or a distribution of possible outputs, which can be crucial depending on the application.</p> </ul> </ol></div> <h4 id=different_kinds_of_text_processing_and_probabilistic_techniques_summarized ><a href="#different_kinds_of_text_processing_and_probabilistic_techniques_summarized" class=header-anchor >Different Kinds of Text Processing and Probabilistic Techniques Summarized</a></h4> <p>For the first two decades or so, the main goal of statistical NLP has been to assign tags, tag sequences, syntactic trees, or use statistical language translations as linguistic inputs to models trained on large corpora of observed linguistic usage.</p> <p>More fully, the types of tasks addressed can be roughly grouped as follows &#40;where additional keywords indicate typical applications&#41;:</p> <div class=colbox-blue ><p><strong>Text and Document Categorization</strong>: </p> <ul> <li><p>authorship, news categories, sentiment analysis;</p> </ul> <p><strong>Context Categorization of Selected Text</strong>: </p> <ul> <li><p>lexical disambiguation, named entity recognition, multi-word expression recognition;</p> </ul> <p><strong>Sequence Tagging</strong>: </p> <ul> <li><p>cell phones → acoustic features → phonemes → words → POS tags;</p> </ul> <p><strong>Structural Assignment of Sentences</strong>: </p> <ul> <li><p>parsing, semantic role labeling, quantifier scoping;</p> </ul> <p><strong>Sentence Transduction</strong>: </p> <ul> <li><p>machine translation, low-frequency computation;</p> </ul> <p><strong>Structural Assignment of Multi-Sentence Texts</strong>: </p> <ul> <li><p>discourse relationships, recall, plan recognition;</p> </ul> <p><strong>Large-Scale Relational Extraction</strong>:</p> <ul> <li><p>knowledge extraction, paraphrase and implication relations;</p> </ul></div> <p><strong>TF-IDF Normalization:</strong> In text classification, one common technique for normalizing word frequencies is TF-IDF &#40;Term Frequency-Inverse Document Frequency&#41;. TF-IDF assigns weights to words based on their frequency in the document and their rarity across the entire corpus. Preprocessing steps such as removing stop words and punctuation are often applied before calculating TF-IDF scores to improve accuracy.</p> <p><strong>Bayesian Approach:</strong> A simple Bayesian approach to text classification assumes that each class generates independent feature values. The probability of observing a set of features given a class is modeled, and the class with the highest posterior probability is selected. While independence assumptions are commonly made between features, more sophisticated models can capture feature interactions for improved classification accuracy.</p> <p><strong>Sequence Models and HMMs:</strong> Sequence markers play a crucial role in text classification by influencing the classification of neighboring words. Generative sequence models like Hidden Markov Models &#40;HMMs&#41; are used to capture these dependencies. For tasks such as part-of-speech &#40;POS&#41; tagging, HMMs estimate the probability of observing a sequence of POS tags given a sequence of words, allowing for more accurate classification.</p> <p><strong>kNN Method and Decision Trees:</strong> The k-Nearest Neighbor &#40;kNN&#41; method assigns class labels based on the majority vote of the k nearest neighbors in the feature space. Decision trees use information theoretic techniques to select classes and branches, providing insight into important features. While decision trees tend to converge to a non-global optimum, this limitation can be mitigated by decision forests.</p> <p><strong>MaxEnt &#40;Logistic Regression&#41; and SVM:</strong> MaxEnt models, also known as Maximum Entropy models or polynomial logistic regression, handle continuous features and provide a flexible framework for classification. Support Vector Machines &#40;SVMs&#41; allow for the distinction of arbitrary class configurations by projecting original class vectors into a higher-dimensional space where classes are linearly divisible. SVMs are mediated by a kernel function, which measures similarity between pairs of vectors, but they do not expand the classification criteria.</p> <p>In summary, each method for text classification has its advantages and disadvantages, depending on factors such as parameter selection, probabilistic model development, and the characteristics of the dataset. Understanding the strengths and weaknesses of each method is crucial for selecting the most appropriate technique for a given NLP task.</p> <h3 id=extending_to_large_language_models_and_deep_neural_networks_in_the_20th_century ><a href="#extending_to_large_language_models_and_deep_neural_networks_in_the_20th_century" class=header-anchor >Extending to Large Language Models and Deep Neural Networks in the 20th Century</a></h3> <p>We have talked so much about the statistical methods in dealing with natural language processing tasks. With respect to parsing, there&#39;s also the development of different neural networks &#40;GAN, ANN, RNNs, etc&#41;. Traditionally recurrent neural networks and their variants have been used extensively for Natural Language Processing problems. In recent years, transformers have outperformed most RNN models. Before looking at transformers, let&#39;s first introduce recurrent neural networks, how they work, and where they fall behind.</p> <p>Recurrent Neural Networks &#40;RNN&#41; operate on sequential data, including time-series data and language translation. Recurrent neural networks come in several forms.</p> <pre><code class="plaintext hljs">Vector to Sequence: 
    takes in a vector and output a sequence of any length.

Sequence to Vector: 
    accepts a sequence as input and output a vector (frequently 
    used for semantic analysis).

Sequence To Sequence: 
    Models take one sequence as input and produce another sequence 
    (mostly used for machine translation).</code></pre> <h4 id=the_encoder-decoder_architecture ><a href="#the_encoder-decoder_architecture" class=header-anchor >The Encoder-Decoder Architecture</a></h4> <p>All of the information from the input sentence will be summarized by encoders, and the decoder will use the encoder&#39;s output to produce the desired output. The encoder&#39;s final state transmits information needed to begin decoding. The decoder calculates a new hidden state and word using the previous state and the output. Both the encoder and decoder layers employ a number of RNNs.</p> <p>But there are drawbacks to recurrent neural networks.</p> <blockquote> <p>First off, they take a long time to train—extremely long time—and frequently we have to cut training short using methods like truncated back propagation in time.</p> <p>Second, and more frequently, RNNs have disappearing and ballooning gradient issues. The information at the beginning of the sentence is lost when used to solve NLP issues.</p> </blockquote> <p>💭 In a separate blog, these different architectures will be introduced in more detail.</p> <h4 id=attention_is_all_you_need ><a href="#attention_is_all_you_need" class=header-anchor >Attention Is All You Need</a></h4> <p>To address some of the issues with conventional RNNs and LSTMs, an attention mechanism was introduced to them. The context vector, which contains the weighted sum of each hidden state in the encoder, served as the basis for the attention mechanism.</p> <p>The context vector describes the relationship between the global input sequence and the decoder&#39;s current state. While the attention method addressed some of the RNN&#39;s intrinsic problems, we were still feeding words one at a time and processing them sequentially, which prevented us from using the parallel processing capabilities of modern hardware.</p> <p><em>Self-Attention</em>, or how each word in a sequence is related to other words in the same sequence, is the main emphasis of the multi-headed attention block. An attention vector created within the attention block serves as a representation for the self-attention. The goal is to identify the contextual connections between the sentence&#39;s words.</p> <p>ℹ️ How does that function? By using the scaled dot products, we may determine the relationship between two vectors.</p> <table><tr><th align=center >The Transformer Model Architecture<tr><td align=center ><img src="../extras/connectionism/trnsfmr.png" alt="" /></table> <p>The mathematical dot product determines how similar two vectors are. In conclusion, two vectors have no correlation if the dot product is 0 and are tightly associated if the dot product is 1 &#40;or -1 in the case of negative correlation&#41;.</p> <p>Our transformer model calculates the attention using a scaled dot-product function. The entire model architecture will be detailed in a separate blog, and here it&#39;s only introduced briefly to support our discussion. Also other model extensions will be introduced as well such as BERT, GPT, Elmo, CNN, LSTM, VAE, GAN, etc.</p> <h4 id=in_a_nutshell ><a href="#in_a_nutshell" class=header-anchor >In A Nutshell</a></h4> <p>The traditional view of language understanding emphasizes the <strong>explicit</strong> use of syntactic, semantic, pragmatic, and world knowledge in deriving meaning, rather than probabilistic methods. It suggests that non-probabilistic formulations are crucial for language understanding and use, and that ambiguity resolution can be achieved through procedures that elaborate on heuristics to facilitate syntactic and semantic interpretations.</p> <p>From a philosophical point of view, this shift is important, not just a practical one: it suggests that traditional thinking about language may rely too much on introspection. The limitations of introspection are that very little of what happens in our brains as we understand or think about language is approached consciously.</p> <p>We consciously register the results of our understanding and thinking, apparently in symbolic form, but not in the process of understanding and thinking itself; and these symbolic abstractions, which to some extent lack a quantitative or probabilistic dimension, can lead us to assume that the underlying processing is also non-quantitative. But the success of statistical NLP, as well as recent developments in cognitive science has argued that language and thought are not only symbolic, but <strong>deeply quantitative, and especially probabilistic</strong>.</p> <p>It was also stimulated by the growing recognition of the distributional nature of language, the emergence of powerful new statistical-based learning methods, and the expectation that these techniques could overcome the scalability issues that had plagued computational linguistics and, more generally, AI from its early days.</p> <p>The corpus-based approach has indeed been quite successful in producing comprehensive, moderately accurate <code>speech recognizers, part-of-speech &#40;POS&#41; taggers, parsers for learned probabilistic phrase-structure grammars, and even MT and text-based QA systems and summarization systems</code>. However, semantic processing has been restricted to rather shallow aspects, such as extraction of specific data concerning specific kinds of events from text or extraction of clusters of argument types, relational tuples, or paraphrase sets from text corpora.</p> <p>Currently, the corpus-based, statistical approaches are still dominant, but there appears to be a growing movement towards integration of formal logical approaches to language with corpus-based statistical approaches in order to achieve deeper understanding and more intelligent behavior in language comprehension and dialogue systems. There are also efforts to <strong>combine connectionist and neural-net approaches with symbolic and logical ones</strong>.</p> <p><strong>References</strong></p> <p><em>Schubert, Lenhart, &quot;Computational Linguistics&quot;, The Stanford Encyclopedia of Philosophy &#40;Spring 2020 Edition&#41;, Edward N. Zalta &#40;ed.&#41;</em></p> <p><em>Muntilde;oz, Eduardo. “Attention Is All You Need: Discovering the Transformer Paper.” Medium, Towards Data Science, 11 Feb. 2021, towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634.</em></p> <p><em>Catherine L. Harris &#40;Auth.&#41;, Noel Sharkey &#40;Eds.&#41;. &#40;n.d.&#41;. Connectionist Natural Language Processing Readings from Connection Science-Springer Netherlands.</em></p> <p><em>Rescorla, Michael, &quot;The Language of Thought Hypothesis&quot;, The Stanford Encyclopedia of Philosophy &#40;Winter 2023 Edition&#41;, Edward N. Zalta &amp; Uri Nodelman &#40;eds.&#41;, https://plato.stanford.edu/archives/win2023/entries/language-thought.</em></p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>