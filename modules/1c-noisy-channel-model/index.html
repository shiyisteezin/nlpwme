<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h1 id=how_everything_started ><a href="#how_everything_started" class=header-anchor >How Everything Started</a></h1> <p>Table of Contents</p> <div class=franklin-toc ><ol><li><a href="#what_exactly_is_a_noisy_channel_model">What Exactly Is A Noisy Channel Model?</a><li><a href="#one_of_the_most_important_masters_theses_in_the_20th_century">One of The Most Important Master&#39;s Theses In The 20th Century</a><li><a href="#the_morse_code">The Morse Code</a><li><a href="#another_important_equation">Another Important Equation</a><li><a href="#bayes_to_the_rescue">Bayes To The Rescue</a></ol></div> <p>Hi there, if you have come across this blog and have the patience to go through it with me, by the end of it you will get to know and understand what noisy channel model that Claude E. Shannon first presented and experimented with back in the 40s really is. And you will also get to understand why it is important for our discussion. </p> <div id=videoContainer  > <div id=player ></div> </div> <script> var tag = document.createElement('script'); tag.src = "https://www.youtube.com/iframe_api"; var firstScriptTag = document.getElementsByTagName('script')[0]; firstScriptTag.parentNode.insertBefore(tag, firstScriptTag); var player; function onYouTubeIframeAPIReady() { player = new YT.Player('player', { width: '100%', position: 'relative', left: '0px', videoId: 'zjWXLD_ihOc', playerVars: { 'autoplay': 0, 'rel': 0, 'cc_load_policy': 1 } }); } function changeYouTubeSource(startTime, endTime) { var youtubeIframe = document.getElementById('player'); var youtubeIframeSrc = document.getElementById('player').getAttribute('src'); var trimmedIframeUrl = ''; var iframeUrlTimeStamp = ''; if (youtubeIframeSrc.match(/&start=/g)) { var mediaFragmentIndex = youtubeIframeSrc.indexOf('&start='); trimmedIframeUrl = youtubeIframeSrc.slice(0, mediaFragmentIndex); if (endTime === 0) { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime; } else { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime + '&end=' + endTime; } } if (youtubeIframeSrc.match(/&start=/g) === null) { if (endTime === 0) { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime; } else { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime + '&end=' + endTime; } } setTimeout(function() { var iframeAutoplayUrl = iframeUrlTimeStamp.replace('autoplay=0', 'autoplay=1'); youtubeIframe.setAttribute('src', iframeAutoplayUrl); }, 1000); } </script> <h3 id=what_exactly_is_a_noisy_channel_model ><a href="#what_exactly_is_a_noisy_channel_model" class=header-anchor >What Exactly Is A Noisy Channel Model?</a></h3> <p>It&#39;s a system designed to capture how information gets transmitted. It&#39;s also a mathematical or probabilistic model developed to capture the way signals get transmitted. </p> <h3 id=one_of_the_most_important_masters_theses_in_the_20th_century ><a href="#one_of_the_most_important_masters_theses_in_the_20th_century" class=header-anchor >One of The Most Important Master&#39;s Theses In The 20th Century</a></h3> <p>A Symbolic Analysis of Relay and Switching Circuits. Shannon&#39;s paper examines symbolic logic of 19th century English mathematician George Boole, and presents how the boolean logic could have a profound impact on electronic circuit design. Which is the entropy equation </p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mtext>H</mtext><mo>=</mo><mo>−</mo><mo>∑</mo><msub><mtext>p</mtext><mi>i</mi></msub><msub><mtext>log</mtext><mi>i</mi></msub><msub><mtext>p</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{H} = -\sum \text{p}_{i} \text{log}_{i} \text{p}_{i}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord text"><span class=mord >H</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.6em;vertical-align:-0.55em;"></span><span class=mord >−</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mop op-symbol large-op" style="position:relative;top:0em;">∑</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord text"><span class=mord >p</span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2441em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord text"><span class=mord >log</span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2441em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord text"><span class=mord >p</span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2441em;"><span></span></span></span></span></span></span></span></span></span></span> <p>The equation we have discussed in another blog defines the information source in terms of the probability distribution of symbols being produced by that information source. </p> <p>This equation marks the fundamental mechanism behind it, which has to do with the level of uncertainty in information. </p> <h3 id=the_morse_code ><a href="#the_morse_code" class=header-anchor >The Morse Code</a></h3> <p>The model is created by having a message written in English encoded in Morse code. The message then gets trasmitted over telegraph line. There&#39;s the noise coming from the transmission line. At the receiving end, another telegraph line receives the message and decodes the message. The decoded message then gets stored as human readable form. The final message may contain errors. </p> <p>Let&#39;s break down the components of the noisy channel model,</p> <ul> <li><p>The model begins with an information source that&#39;s written in human language. </p> <li><p>The message is encoded by the transmitter from English in to Morse code then sent from the channel to the telegraph line. </p> <li><p>The message then gets sent to the receiver to the receiver for decoding. </p> <li><p>Because of the noise in the channel, the final decoded message may contain errors.</p> </ul> <h3 id=another_important_equation ><a href="#another_important_equation" class=header-anchor >Another Important Equation</a></h3> <p>Here I will follow the video and jot down the important equation that represents the transmission of a message through the noisy channel. </p> <p>Below is the equation. So, here <code>e</code> and/or <code>f</code> represents respectively the message that gets encoded and sent in the channel and the output message that gets decoded back to human readable language. </p> <p>Here as we are looking at the equation, given the output message <code>f</code>, we want to reconstruct the message back to <code>e</code>, by finding the <code>e</code> that maximizes the probability of <code>f</code> given <code>e</code>. Whatever <code>e</code> hat that satisfies our conditions will be the best hypothesis. </p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mover accent=true ><mi>e</mi><mo>^</mo></mover><mo>=</mo><mi><munder><mo><mi>arg</mi><mo>⁡</mo><mi>max</mi><mo>⁡</mo></mo><mi>e</mi></munder></mi><mtext> </mtext><mi>p</mi><mo stretchy=false >(</mo><mtext>e|f</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\hat{e} = \underset{e}{\arg\max} \ p(\text{e|f})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.6944em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.1944em;"><span class=mord >^</span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.6444em;vertical-align:-0.8944em;"></span><span class=mord ><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.4306em;"><span style="top:-2.2056em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop ><span class=mop >ar<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >max</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8944em;"><span></span></span></span></span></span></span><span class=mspace > </span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >e|f</span></span><span class=mclose >)</span></span></span></span></span> <h3 id=bayes_to_the_rescue ><a href="#bayes_to_the_rescue" class=header-anchor >Bayes To The Rescue</a></h3> <p>To calculate the probability, we are going to use an important mathematical law, the Bayes equation. </p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mtext>e|f</mtext><mo stretchy=false >)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy=false >(</mo><mtext mathvariant=bold >f|e</mtext><mo stretchy=false >)</mo><mi>p</mi><mo stretchy=false >(</mo><mtext mathvariant=bold >e</mtext><mo stretchy=false >)</mo></mrow><mtext mathvariant=bold >f</mtext></mfrac></mrow><annotation encoding="application/x-tex">p(\text{e|f}) = \frac{p(\textbf{f|e})p(\textbf{e})}{\textbf{f}} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >e|f</span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:2.113em;vertical-align:-0.686em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.427em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord text"><span class="mord textbf">f</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class="mord textbf">f|e</span></span><span class=mclose >)</span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class="mord textbf">e</span></span><span class=mclose >)</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> <p>We will be talking about this God of an equation independently in another blog, but here we could understand that we are able to calculate the above noisy channel model through using the Bayes&#39; law. And the three components available in the equation will allow us to do this. </p> <p>The posterior function which is intuitively the outcome function <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mtext>e|f</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(\text{e|f})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >e|f</span></span><span class=mclose >)</span></span></span></span> is derived from the likelihood function which is the probability of <code>e</code> given <code>f</code> or <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mtext>f|e</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(\text{f|e})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >f|e</span></span><span class=mclose >)</span></span></span></span> and the prior distributions <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi>e</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(e)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal">e</span><span class=mclose >)</span></span></span></span> together divided by the prior distribution of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi>f</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(f)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mclose >)</span></span></span></span>.</p> <p>Given this definition, we could redefine our previous definition in terms of the channel model and the language models.</p> <p>First we do it by taking the posterior solution <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mtext>e|f</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(\text{e|f})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >e|f</span></span><span class=mclose >)</span></span></span></span> and replace it with the right-hand side of the Bayes&#39; Law. According to the tutorial video, we could simplify the equation a little bit. </p> <p>Recall performing an arg max operation over English messages <code>e</code>, this means we are looking for the English message <code>e</code>, which maximizes the total value of the equation. </p> <p>Since <code>e</code> doesn&#39;t show up in the denominator, so no matter what value <code>e</code> has, won&#39;t be affected which leaves us with the final equation,</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mover accent=true ><mi>e</mi><mo>^</mo></mover><mo>=</mo><mi><munder><mo><mi>arg</mi><mo>⁡</mo><mi>max</mi><mo>⁡</mo></mo><mi>e</mi></munder></mi><mo stretchy=false >(</mo><mtext>f|e</mtext><mo stretchy=false >)</mo><mtext> </mtext><mi>p</mi><mo stretchy=false >(</mo><mtext>e</mtext><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\hat{e} = \underset{e}{\arg\max} (\text{f|e}) \ p(\text{e})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.6944em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathnormal">e</span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.1944em;"><span class=mord >^</span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.6444em;vertical-align:-0.8944em;"></span><span class=mord ><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.4306em;"><span style="top:-2.2056em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop ><span class=mop >ar<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >max</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8944em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord text"><span class=mord >f|e</span></span><span class=mclose >)</span><span class=mspace > </span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class="mord text"><span class=mord >e</span></span><span class=mclose >)</span></span></span></span></span> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>