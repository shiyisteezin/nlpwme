<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=generative_pre-trained_transformer_a_lightweight_introduction ><a href="#generative_pre-trained_transformer_a_lightweight_introduction" class=header-anchor >Generative Pre-trained Transformer: A Lightweight Introduction </a></h2> <div class=franklin-toc ><ol><li><a href="#generative_pre-trained_transformer_a_lightweight_introduction">Generative Pre-trained Transformer: A Lightweight Introduction </a><li><a href="#key_tasks_related_to_the_model">Key Tasks Related to The Model</a><li><a href="#graphic_representation_of_the_model_architecture">Graphic Representation of The Model Architecture</a></ol></div> <p>Transformers are a type of deep learning model architecture introduced in the paper &quot;Attention is All You Need&quot; by Vaswani et al. in 2017. They have become a fundamental building block for various natural language processing &#40;NLP&#41; tasks due to their effectiveness in capturing contextual information and dependencies in sequential data. Transformers have significantly contributed to the state-of-the-art performance in a wide range of NLP applications.</p> <p>Key components of transformers include:</p> <table><tr><th align=center >1. <strong>Self-Attention Mechanism:</strong><tr><td align=center >The core innovation of transformers is the self-attention mechanism. It allows the model to weigh the importance of different words in a sequence when encoding or decoding, considering the entire context rather than fixed-size context windows.</table> <table><tr><th align=center >2. <strong>Multi-Head Attention:</strong><tr><td align=center >Transformers use multiple attention heads in parallel, allowing the model to learn different relationships between words in parallel. This helps capture various types of dependencies in the data.</table> <table><tr><th align=center >3. <strong>Positional Encoding:</strong><tr><td align=center >Since transformers do not inherently understand the sequential order of the input, positional encoding is added to the input embeddings to provide information about the position of each token in the sequence.</table> <table><tr><th align=center >4. <strong>Encoder and Decoder Layers:</strong><tr><td align=center >Transformers consist of a stack of encoder and decoder layers. The encoder processes the input sequence, while the decoder generates the output sequence. Each layer contains self-attention mechanisms and feedforward neural networks.</table> <table><tr><th align=center >5. <strong>Feedforward Neural Networks:</strong><tr><td align=center >Transformers use feedforward neural networks to process information within each position independently. This helps in capturing complex patterns and relationships.</table> <table><tr><th align=center >6. <strong>Layer Normalization and Residual Connections:</strong><tr><td align=center >Layer normalization and residual connections are employed to stabilize and speed up training. Residual connections allow the model to skip certain layers, facilitating the flow of information.</table> <table><tr><th align=center >7. <strong>Attention Masks:</strong><tr><td align=center >Attention masks are used to control which positions in the input sequence are attended to. For instance, during language modeling, the model attends to all positions before a given position but not after.</table> <h2 id=key_tasks_related_to_the_model ><a href="#key_tasks_related_to_the_model" class=header-anchor >Key Tasks Related to The Model</a></h2> <p>Transformers have achieved significant success in various NLP tasks, including:</p> <div class=colbox-blue ><ul> <li><p><strong>Machine Translation:</strong> Transformers have been particularly successful in the field of machine translation, outperforming previous sequence-to-sequence models.</p> <li><p><strong>Text Generation:</strong> They are used in generating coherent and contextually relevant text, as seen in models like OpenAI&#39;s GPT &#40;Generative Pre-trained Transformer&#41; series.</p> <li><p><strong>Named Entity Recognition &#40;NER&#41;:</strong> Transformers are effective in tasks where understanding contextual dependencies is crucial, such as named entity recognition.</p> <li><p><strong>Text Classification:</strong> For tasks like sentiment analysis or document categorization, transformers can effectively capture context and relationships between words.</p> </ul></div> <p>Prominent transformer-based models include BERT &#40;Bidirectional Encoder Representations from Transformers&#41;, GPT &#40;Generative Pre-trained Transformer&#41;, T5 &#40;Text-To-Text Transfer Transformer&#41;, and more. These models are often pre-trained on large corpora and fine-tuned for specific downstream tasks.</p> <h2 id=graphic_representation_of_the_model_architecture ><a href="#graphic_representation_of_the_model_architecture" class=header-anchor >Graphic Representation of The Model Architecture</a></h2> <table><tr><th align=center >The Model Architecture<tr><td align=center ><img src="../extras/connectionism/trnsfmr.png" alt="" /></table> <p>This image depicts the architecture of the Transformer model, which is widely used in natural language processing tasks. Here&#39;s a step-by-step analysis:</p> <div class=colbox-blue ><ul> <li><p><strong>Inputs</strong> are first converted into <strong>Input Embeddings</strong>, and <strong>Positional Encodings</strong> are added to provide sequence information.</p> <li><p>The <strong>Encoder</strong> consists of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">N_x</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> identical layers, each with two sub-layers: <strong>Multi-Head Attention</strong> and <strong>Feed Forward</strong> neural network. Each sub-layer has a residual connection around it, followed by layer normalization &#40;<strong>Add &amp; Norm</strong>&#41;.</p> <li><p>The <strong>Decoder</strong> also has <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">N_x</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> identical layers, but with an additional <strong>Masked Multi-Head Attention</strong> layer to prevent positions from attending to subsequent positions. This is crucial for predicting the next word in a sequence.</p> <li><p>The <strong>Output Embedding</strong> &#40;shifted right&#41; is similarly added with <strong>Positional Encodings</strong> and passed through the decoder layers.</p> <li><p>The final output of the decoder passes through a <strong>Linear</strong> layer and a <strong>Softmax</strong> function to produce <strong>Output Probabilities</strong>, which can be used for tasks like translation, text generation, etc.</p> </ul></div> <p>The Transformer model is known for its parallelization capabilities and efficiency in handling long-range dependencies in text.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>