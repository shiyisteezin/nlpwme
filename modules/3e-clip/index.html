<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=clip_or_the_contrastive_language-image_pre-training ><a href="#clip_or_the_contrastive_language-image_pre-training" class=header-anchor >CLIP or The Contrastive Language-Image Pre-training </a></h2> <p>Here&#39;s a breakdown of CLIP &#40;Contrastive Language-Image Pre-training&#41; along with a simple coding example:</p> <h3 id=clip_breakdown ><a href="#clip_breakdown" class=header-anchor >CLIP Breakdown:</a></h3> <ol> <li><p><strong>Contrastive Learning Objective</strong>:</p> <ul> <li><p>CLIP is trained using a contrastive learning approach, where it learns to associate images and corresponding textual descriptions. The model is trained to maximize agreement between image-text pairs and minimize agreement between mismatched pairs.</p> </ul> <li><p><strong>Unified Vision-Text Embedding Space</strong>:</p> <ul> <li><p>CLIP learns a shared embedding space for both images and text, allowing it to represent visual and textual information in a common feature space. This enables the model to perform various tasks involving image-text interactions.</p> </ul> <li><p><strong>Vision Transformer &#40;ViT&#41; Backbone</strong>:</p> <ul> <li><p>CLIP utilizes a vision transformer &#40;ViT&#41; backbone for processing images. ViT divides the input image into patches, which are then linearly embedded and processed by transformer layers, enabling the model to capture spatial relationships in the image.</p> </ul> <li><p><strong>Text Encoding</strong>:</p> <ul> <li><p>CLIP encodes textual descriptions using a transformer-based architecture similar to T5. It processes the text input through stacked transformer layers to extract meaningful representations of text.</p> </ul> <li><p><strong>Cross-modal Alignment</strong>:</p> <ul> <li><p>CLIP learns to align the representations of images and text in the shared embedding space. This allows the model to perform tasks such as image classification, image retrieval, and image generation based on textual prompts.</p> </ul> </ol> <h3 id=simple_coding_example ><a href="#simple_coding_example" class=header-anchor >Simple Coding Example:</a></h3> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >import</span> clip

<span class=hljs-comment ># Load pre-trained CLIP model</span>
device = <span class=hljs-string >&quot;cuda&quot;</span> <span class=hljs-keyword >if</span> torch.cuda.is_available() <span class=hljs-keyword >else</span> <span class=hljs-string >&quot;cpu&quot;</span>
model, preprocess = clip.load(<span class=hljs-string >&quot;ViT-B/32&quot;</span>, device=device)

<span class=hljs-comment ># Example image and text</span>
image = preprocess(torch.randn(<span class=hljs-number >3</span>, <span class=hljs-number >224</span>, <span class=hljs-number >224</span>)).unsqueeze(<span class=hljs-number >0</span>).to(device)
text = clip.tokenize([<span class=hljs-string >&quot;a photo of a cat&quot;</span>]).to(device)

<span class=hljs-comment ># Perform image-text embedding</span>
<span class=hljs-keyword >with</span> torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

<span class=hljs-comment ># Calculate cosine similarity between image and text features</span>
similarity = (image_features @ text_features.T).squeeze(<span class=hljs-number >0</span>)

<span class=hljs-comment ># Print similarity score</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Similarity score:&quot;</span>, similarity.item())</code></pre> <p>In this example, we load a pre-trained CLIP model and preprocess an example image and text. We then encode both the image and text into feature vectors using the model&#39;s <code>encode_image</code> and <code>encode_text</code> functions, respectively. Finally, we calculate the cosine similarity between the image and text features to measure their semantic similarity.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>