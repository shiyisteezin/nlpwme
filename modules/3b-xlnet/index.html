<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/katex/katex.min.css"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><h2 id=xlnet_generalized_autoregressive_pretraining_for_language_understanding ><a href="#xlnet_generalized_autoregressive_pretraining_for_language_understanding" class=header-anchor >XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h2> <p>XLNet &#40;eXtreme Learning Machine&#41; is a state-of-the-art language model developed by Google AI. It builds upon the Transformer architecture, which is a deep learning model specifically designed for sequence-to-sequence tasks such as language modeling.</p> <p><a href="https://arxiv.org/abs/1906.08237">Here</a> is the original link to the paper. </p> <p>Here&#39;s a breakdown of XLNet along with necessary formulas, explanations, and potential code examples:</p> <ol> <li><p><strong>Transformer Architecture</strong>: XLNet utilizes the Transformer architecture as its backbone. The key components of the Transformer architecture include:</p> <p>a. <strong>Self-Attention Mechanism</strong>: This mechanism allows the model to weigh the importance of each word/token in the context of the entire sequence. It computes attention scores between all pairs of words in a sequence and uses these scores to create context-aware representations for each word.</p> <p>b. <strong>Positional Encoding</strong>: Since Transformers do not inherently understand the order of words in a sequence, positional encodings are added to the input embeddings to provide positional information to the model.</p> <p>c. <strong>Feedforward Neural Networks</strong>: After obtaining contextualized representations through self-attention, Transformer layers typically pass the representations through feedforward neural networks to capture more complex patterns.</p> <li><p><strong>Permutation Language Modeling &#40;PLM&#41;</strong>: XLNet introduces Permutation Language Modeling, which differs from traditional autoregressive language modeling used in models like GPT &#40;Generative Pre-trained Transformer&#41;. In PLM, instead of conditioning on previous words sequentially, the model conditions on all permutations of the input tokens. This allows XLNet to learn bidirectional relationships between tokens.</p> <li><p><strong>Training Objective</strong>: XLNet uses a modified version of the autoregressive training objective used in models like GPT. The objective is to maximize the expected log-likelihood of the target sequence given the input sequence. Mathematically, it can be represented as:</p> </ol> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>θ</mi></munder><msub><mi mathvariant=double-struck >E</mi><mrow><mo stretchy=false >(</mo><mi>x</mi><mo separator=true >,</mo><mi>y</mi><mo stretchy=false >)</mo><mo>∼</mo><mi mathvariant=script >D</mi></mrow></msub><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>y</mi></msub></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=false >(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=normal >∣</mi><mi>x</mi><mo separator=true >,</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator=true >;</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \max_{\theta} \mathbb{E}_{(x,y) \sim \mathcal{D}} \sum_{t=1}^{T_y} \log P(y_t | x, y_{&lt; t}; \theta) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:3.2038em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.4306em;"><span style="top:-2.3479em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop >max</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.7521em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3552em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.9367em;"><span style="top:-1.8829em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.4083em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.2671em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class="mord mathnormal">x</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1774em;"><span></span></span></span></span></span></span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span></span> <p>Where:</p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi>x</mi><mo separator=true >,</mo><mi>y</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> (x, y) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mclose >)</span></span></span></span> represents a training example consisting of an input sequence <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> and its corresponding target sequence <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex"> y </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>.</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex"> T_y </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.9694em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> is the length of the target sequence.</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex"> \theta </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> represents the model parameters.</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy=false >(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=normal >∣</mi><mi>x</mi><mo separator=true >,</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator=true >;</mo><mi>θ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> P(y_t | x, y_{&lt; t}; \theta) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class="mord mathnormal">x</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1774em;"><span></span></span></span></span></span></span><span class=mpunct >;</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class=mclose >)</span></span></span></span> is the conditional probability of the target token <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex"> y_t </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> given the input sequence <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> and previously generated tokens <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> y_{&lt; t} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1774em;"><span></span></span></span></span></span></span></span></span></span>.</p> </ul> <ol start=4 > <li><p><strong>Implementation</strong>: XLNet can be implemented using various deep learning frameworks such as TensorFlow or PyTorch. Below is a simplified PyTorch code snippet demonstrating how to use XLNet for text generation:</p> </ol> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> XLNetLMHeadModel, XLNetTokenizer

<span class=hljs-comment ># Load pre-trained XLNet model and tokenizer</span>
model_name = <span class=hljs-string >&#x27;xlnet-base-cased&#x27;</span>
tokenizer = XLNetTokenizer.from_pretrained(model_name)
model = XLNetLMHeadModel.from_pretrained(model_name)

<span class=hljs-comment ># Input text</span>
input_text = <span class=hljs-string >&quot;The cat sat on the&quot;</span>

<span class=hljs-comment ># Tokenize input text</span>
input_ids = tokenizer.encode(input_text, add_special_tokens=<span class=hljs-literal >False</span>, return_tensors=<span class=hljs-string >&quot;pt&quot;</span>)

<span class=hljs-comment ># Generate text using XLNet</span>
max_length = <span class=hljs-number >50</span>
output = model.generate(input_ids, max_length=max_length, num_return_sequences=<span class=hljs-number >1</span>)

<span class=hljs-comment ># Decode generated tokens</span>
generated_text = tokenizer.decode(output[<span class=hljs-number >0</span>], skip_special_tokens=<span class=hljs-literal >True</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Generated text:&quot;</span>, generated_text)</code></pre> <p>This code demonstrates how to generate text using a pre-trained XLNet model. You first need to load the model and tokenizer, tokenize your input text, generate text using the model&#39;s <code>generate</code> method, and finally decode the generated tokens to obtain the output text.</p> <p>This breakdown provides a high-level overview of XLNet, its key components, training objective, and a simple code example for text generation. For more advanced usage and fine-tuning for specific tasks, additional considerations and modifications may be required.</p> <h2 id=bert_vs_xlnet ><a href="#bert_vs_xlnet" class=header-anchor >BERT vs XLNet</a></h2> <p>BERT &#40;Bidirectional Encoder Representations from Transformers&#41; and XLNet are both state-of-the-art language models, but they differ in their approach to handling bidirectionality and context modeling. Here&#39;s an explanation of how BERT lacks bidirectional context modeling and how XLNet addresses this issue:</p> <ol> <li><p><strong>BERT&#39;s Masked Language Model &#40;MLM&#41;</strong>:</p> <ul> <li><p>BERT is pre-trained using a Masked Language Model objective, where a percentage of the input tokens are randomly masked, and the model is trained to predict these masked tokens based on the surrounding context.</p> <li><p>While BERT captures bidirectional context within a single training instance &#40;i.e., it can see both left and right context during pre-training&#41;, it lacks the ability to capture bidirectional dependencies across multiple training instances. This is because each training instance &#40;sentence or segment&#41; is processed independently.</p> </ul> <li><p><strong>XLNet&#39;s Permutation Language Modeling &#40;PLM&#41;</strong>:</p> <ul> <li><p>XLNet, on the other hand, introduces Permutation Language Modeling, which is an improvement over BERT&#39;s approach. Instead of masking tokens as in BERT, XLNet conditions on all possible permutations of the input tokens.</p> <li><p>By considering all permutations, XLNet can learn bidirectional relationships between tokens across multiple training instances. This enables XLNet to capture richer contextual information and dependencies compared to BERT.</p> <li><p>Furthermore, XLNet maintains BERT&#39;s ability to capture bidirectional context within a single training instance by considering all permutations of the input tokens, including the original order.</p> </ul> <li><p><strong>Improvements by XLNet</strong>:</p> <ul> <li><p>XLNet&#39;s PLM addresses the limitations of BERT by explicitly modeling bidirectional dependencies across multiple training instances.</p> <li><p>XLNet achieves this by considering all possible permutations of the input tokens during training, allowing it to capture a more comprehensive understanding of the text&#39;s context.</p> <li><p>As a result, XLNet tends to outperform BERT on various downstream NLP tasks that require a deeper understanding of context and dependencies across sentences or documents.</p> </ul> </ol> <p>In summary, while BERT captures bidirectional context within a single training instance through masked language modeling, it lacks the ability to model bidirectional dependencies across multiple instances. XLNet addresses this limitation by introducing Permutation Language Modeling, which enables it to capture bidirectional relationships across multiple instances, leading to improved performance on various NLP tasks.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: December 04, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>